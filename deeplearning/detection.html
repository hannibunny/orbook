
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Object Detection &#8212; Object Recognition Lecture</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Face Detection" href="../face/faceDetection.html" />
    <link rel="prev" title="Convolutional Neural Networks for Object Recognition" href="cnns.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/hdmlogomed.jpg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Object Recognition Lecture</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Intro and Overview Object Recognition Lecture
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Image Processing
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../preprocessing/01accessImage.html">
   Basic Image Access Operations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../preprocessing/02filtering.html">
   Basic Filter Operations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../preprocessing/04gaussianDerivatives.html">
   Gaussian Filter and Derivatives of Gaussian
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../preprocessing/03LowPassFilter.html">
   Rectangular- and Gaussian Low Pass Filtering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../preprocessing/06GaussianNoiseReduction.html">
   Noise Suppression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../preprocessing/05GaussianLowPassFilter.html">
   Gaussian and Difference of Gaussian Pyramid
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Features
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../features/globalDescriptors.html">
   Global Image Features
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../features/similarityMetrics.html">
   Similarity Measures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../features/ImageRetrieval.html">
   Histogram-based Image Retrieval
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../features/ImageRetrieval.html#use-pretrained-cnns-for-retrieval">
   Use pretrained CNNs for Retrieval
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../features/multiReceptiveFields.html">
   Multidimensional Receptive Field Histograms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../features/naiveBayesHistogram.html">
   Histogram-based Naive Bayes Recognition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../features/probRecognition.html">
   Example: Naive Bayes Object Recognition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../features/localFeatures.html">
   Local Image Features
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../features/harrisCornerDetection.html">
   Example: Harris-Förstner Corner Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../features/siftDescriptorCV2.html">
   Example: Create SIFT Descriptors with openCV
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../features/HoGfeatures.html">
   Histogram of Oriented Gradients: Step-by-Step
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../features/HOGpedestrianDetection.html">
   HOG-based Pedestrian Detection
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Object Recognition
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../recognition/objectrecognition.html">
   Object Recognition
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Deep Learning
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="ConvolutionNeuralNetworks.html">
   Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="convolutionDemos.html">
   Animations of Convolution and Deconvolution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="cnns.html">
   Convolutional Neural Networks for Object Recognition
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Object Detection
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Face Detection and Recognition
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../face/faceDetection.html">
   Face Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../face/faceRecognition.html">
   Face Recognition using FaceNet
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Pose Estimation
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../poseEstimation/Pose_Estimation.html">
   Multi-Person 2D Pose Estimation using Part Affinity Fields
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../referenceSection.html">
   References
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/deeplearning/detection.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#goal">
     Goal
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#approaches">
     Approaches
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#performance-measure">
     Performance Measure
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#region-proposals">
   Region Proposals
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#r-cnn">
   R-CNN
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inference-process">
     Inference Process
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#training">
     Training
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#r-cnn-performance-and-drawbacks">
     R-CNN Performance and Drawbacks
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#spatial-pyramid-pooling-sppnet">
   Spatial Pyramid Pooling (SPPnet)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#problem-of-fixed-size-input-to-cnn">
     Problem of fixed size input to CNN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sppnet-allows-variable-size-input">
     SPPnet allows variable size input
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fast-r-cnn">
   Fast R-CNN
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inference-process-overview">
     Inference Process Overview
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#roi-pooling-layer">
     RoI Pooling Layer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id10">
     Training
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#apply-pretrained-cnns">
       Apply pretrained CNNs
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#hierarchical-sampling-enables-fine-tuning">
       Hierarchical Sampling enables Fine-Tuning
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#multi-task-loss">
       Multi-Task Loss
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#faster-r-cnn">
   Faster R-CNN
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#rpn">
     RPN
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#roi-pooling-and-r-cnn">
     RoI-Pooling and R-CNN
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="object-detection">
<h1>Object Detection<a class="headerlink" href="#object-detection" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<div class="section" id="goal">
<h3>Goal<a class="headerlink" href="#goal" title="Permalink to this headline">¶</a></h3>
<p>The goal of object detection is to dermine for a given image:</p>
<ul class="simple">
<li><p>Which objects are in the image</p></li>
<li><p>Where are these objects</p></li>
<li><p>Confidence score of detection</p></li>
</ul>
<figure align="center">
  <img src="https://maucher.home.hdm-stuttgart.de/Pics/ILSVRC2014detection.PNG" style="width:600px" align="center">
  <figurecaption>Image Source: <a href="http://image-net.org/challenges/LSVRC/2014/index">http://image-net.org/challenges/LSVRC/2014/index</a></figurecaption>
</figure>
</div>
<div class="section" id="approaches">
<h3>Approaches<a class="headerlink" href="#approaches" title="Permalink to this headline">¶</a></h3>
<p>An old approach for obect detection is sliding window detection. Here a window is slided over the entire image. For each window a classifier determines, which object is contained in the window. In the most simple version only one object-type (e.g. car) is of interest and the binary classifier has to distinguish if this object is contained in the window or not. The corresponding position information provided implicitely by the position of the current window within the image. This approach is depicted below:</p>
<figure align="center">
  <img src="https://maucher.home.hdm-stuttgart.de/Pics/slidingWindow.jpg" style="width:700px" align="center">  
<figcaption>
Sliding window approach for object detection 
</figcaption>
</figure> 
<p>For this approach any supervised Machine-Learning algorithm can be applied, e.g. Support Vector Machines (SVM) or Mult-Layer-Perceptron. The content of a single window is either passed directly, in terms of pixel intensities, to the classifier or represented by other types of manually or automatically extracted features (see <a class="reference internal" href="../features/globalDescriptors.html"><span class="doc std std-doc">section features</span></a>). The sliding window approach is computationally expensive, since windows of different sizes and aspect ratios have to be shifted in a fine-granular manner over the entire image-space</p>
<p>Subject of this section is another approach for object detection: Deep Neural Networks, such as R-CNN and Yolo.</p>
</div>
<div class="section" id="performance-measure">
<h3>Performance Measure<a class="headerlink" href="#performance-measure" title="Permalink to this headline">¶</a></h3>
<p><strong>Intersection over Union (IoU) and mean Average Precision (mAP)</strong></p>
<p>If <span class="math notranslate nohighlight">\(A\)</span> is the set of detected pixels and <span class="math notranslate nohighlight">\(B\)</span> is the set of Groundtruth-pixel, their IoU is defined to be</p>
<div class="math notranslate nohighlight">
\[
IoU(A, B) = \frac{|A \cap B|}{|A \cup B|},
\]</div>
<p>where <span class="math notranslate nohighlight">\(|X|\)</span> is the number of pixels in set <span class="math notranslate nohighlight">\(X\)</span>. Usually two bounding boxes (pixel sets) are said to <strong>match</strong>, if their IoU is <span class="math notranslate nohighlight">\(&gt;0.5\)</span>. The output of the detector is correct, if</p>
<ul class="simple">
<li><p>detected object = groundtruth object, and</p></li>
<li><p>detected bounding box and groundtruth bounding box match ( IoU is <span class="math notranslate nohighlight">\(&gt;0.5\)</span>)</p></li>
</ul>
<p>Based on the correct and erroneous outputs the number of true positives (TP), true negatives (TN), false positives (FP), false negatives (FN) and the <strong>recall</strong> and <strong>precision</strong> can be determined. For a given class the <strong>Average Precision (AP)</strong> is related to the area under the precision-recall curve for a class:</p>
<div class="math notranslate nohighlight">
\[
AP=\frac{1}{11} \sum\limits_{r \in \lbrace 0, 0.1, \ldots, 1 \rbrace} p_{interp}(r),
\]</div>
<p>where <span class="math notranslate nohighlight">\(p_{interp}(r)\)</span> is the interpolated precision at recall <span class="math notranslate nohighlight">\(r\)</span>. The mean of these average individual-class-precisions is the <strong>mean Average Precision (mAP)</strong>. More info can be obtained from <a class="reference external" href="http://homepages.inf.ed.ac.uk/ckiw/postscript/ijcv_voc09.pdf">PASCAL VOC Challenge</a></p>
</div>
</div>
<div class="section" id="region-proposals">
<h2>Region Proposals<a class="headerlink" href="#region-proposals" title="Permalink to this headline">¶</a></h2>
<p>Many Deep Neural Networks for Object Detection, e.g. R-CNN, require region proposals. The task of region proposal methods is to propose a list of bounding boxes within the image, which likely contain an object of interest. In order to determine these regions image-segmentation algorithms are applied, such as hierarchical clustering, mean-shift clustering or graph-based segmentation. The outputs of the applied segmentation process is often refined by methods such as selective search (see e.g. <a class="reference external" href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf">here</a>).</p>
</div>
<div class="section" id="r-cnn">
<h2>R-CNN<a class="headerlink" href="#r-cnn" title="Permalink to this headline">¶</a></h2>
<p>R-CNN (Regions with CNN features) has been published 2014 in <a class="bibtex reference internal" href="../referenceSection.html#girshickdonahuedarrelletal" id="id1">[GDDM14]</a>. It combines <strong>Region Proposals</strong> and <strong>CNNs</strong>. As mentioned above <em>Region Proposals</em> are candidate boxes, which likely contain an object. R-CNN does not require a specific region proposal algorithm. However, in the experiments of <a class="bibtex reference internal" href="../referenceSection.html#girshickdonahuedarrelletal" id="id2">[GDDM14]</a> the authors applied <a class="reference external" href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf">Selective Search</a>).</p>
<div class="section" id="inference-process">
<h3>Inference Process<a class="headerlink" href="#inference-process" title="Permalink to this headline">¶</a></h3>
<p>The overall R-CNN process, as depicted in the image below, consists of the following steps:</p>
<ol class="simple">
<li><p>Apply <strong>Selective Search</strong> for calculating about 2000 region proposals for the given image.</p></li>
<li><p>Warp each region proposal to a fixed size, since the <strong>CNN requires a fixed-size input</strong>.</p></li>
<li><p>Pass each warped region proposal through the feature extractor-part of a CNN (modified AlexNet). The CNN-extractor provides for each region proposal a feature-vector of length 4096 to the following classifier.</p></li>
<li><p><strong>Linear SVM-Classifier</strong> calculates a probability for each class. and each region proposal</p></li>
<li><p>Given all scored regions in an image, a greedy non-maximum suppression is applied (for each class independently) that rejects a region if it has an IoU overlap with a higher scoring selected region larger than a learned threshold.</p></li>
<li><p>Since the region proposals are not accurate, <strong>Bounding-Box Regression</strong> is applied to compute accurate bounding boxes. Input are coordinates of the region proposal. Output are the coordinates of the groundtruth bounding-box.</p></li>
</ol>
<figure align="center"><img src="https://maucher.home.hdm-stuttgart.de/Pics/R-CNN.PNG" style="width:600px" align="center">
<figcaption>
	Image Source: Source: <a href="https://arxiv.org/pdf/1311.2524.pdf">https://arxiv.org/pdf/1311.2524.pdf</a>
</figcaption>
</figure> 
</div>
<div class="section" id="training">
<h3>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h3>
<p>R-CNN consists of 3 modells, which are trained as described below.</p>
<p><strong>Training the CNN-Feature Extractor:</strong></p>
<ul class="simple">
<li><p>The CNN feature extractor is pretrained on the ILSVRC2012 imagenet data.</p></li>
<li><p>The CNN feature extractor is fine-tuned with the data, given in the current task in order to adapt it to the relevant domain. For this the AlexNet classifier, which is designed to distinguish 1000 classes, is replaced by a classifier, which can either distinguish <em>21</em> classes in the case of PASCAL VOC 2010 data (20 classes + background), or <em>201</em> classes in the case of ILSVRC2013.</p></li>
<li><p>Each batch consists of 32 positives and 96 negatives. A region proposal is labeled <em>positive</em> if it’s overlap with a ground-truth box is <span class="math notranslate nohighlight">\(IoU \geq 0.5\)</span>, otherwise background.</p></li>
</ul>
<p><strong>Training the SVM:</strong></p>
<ul class="simple">
<li><p>After training and fine-tuning the CNN-feature extractor, it can be applied to calculate the 4096-dimensional feature vector for each region proposal.</p></li>
<li><p>The feature vectors are applied as the input of the SVM. For each class a binary linear SVM is learned.</p></li>
<li><p>For training SVMs only the ground-truth boxes are taken as <em>positive</em> examples for their respective classes and proposals with less than 0.3 IoU overlap with all instances of a class are labeled as <em>negative</em> for that class.</p></li>
</ul>
<p><strong>Training the Bounding-Box-Regressor:</strong></p>
<p>After scoring each region proposal with a class-specific detection SVM, a new bounding box using a class-specific bounding-box regressor is predicted.</p>
<ul class="simple">
<li><p>Set of <span class="math notranslate nohighlight">\(N\)</span> training pairs <span class="math notranslate nohighlight">\(\{(P^i,G^i)\}\)</span>, where</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
P^i=(P^i_x,P^i_y,P^i_w,P^i_h) \quad \mbox{ and } \quad G^i=(G^i_x,G^i_y,G^i_w,G^i_h)
\]</div>
<p>specify the pixel coordinates of the center, width and height of the region proposal (P) and ground-truth (G) bounding-box, respectively.</p>
<ul class="simple">
<li><p>The goal is to learn a transformation from <span class="math notranslate nohighlight">\(P\)</span> to <span class="math notranslate nohighlight">\(G\)</span> (index <span class="math notranslate nohighlight">\(i\)</span> is omitted ). This transformation is parametrized as follows:</p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-4b784f26-c008-4b6b-929b-171889b7c1e7">
<span class="eqno">(94)<a class="headerlink" href="#equation-4b784f26-c008-4b6b-929b-171889b7c1e7" title="Permalink to this equation">¶</a></span>\[\begin{eqnarray}
\hat{G}_x &amp; = &amp; P_w d_x(P) + P_x \\
\hat{G}_y &amp; = &amp; P_h d_y(P) + P_y \\
\hat{G}_w &amp; = &amp; P_w \exp(d_w(P))\\
\hat{G}_h &amp; = &amp; P_h \exp(d_h(P))\\
\end{eqnarray}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{G}\)</span> is the regressors prediction for <span class="math notranslate nohighlight">\(G\)</span>. Each of the 4 function <span class="math notranslate nohighlight">\(d_*(P)\)</span> is modeled as a linear function</p>
<div class="math notranslate nohighlight">
\[
d_*(P)= w_*^T \phi(P)
\]</div>
<p>of the output <span class="math notranslate nohighlight">\(\phi(P)\)</span> of the last pooling layer of the CNN-extractor. The weight vectors <span class="math notranslate nohighlight">\(w_*^T\)</span> are learned by Ridge Regression:</p>
<div class="math notranslate nohighlight">
\[
w_* = \operatorname*{argmin}_{\hat{w}_*} \sum\limits_{i=1}^N (t^i - \hat{w}_* \phi(P^i))^2 + \lambda \mid\mid \hat{w}_* \mid\mid^2,
\]</div>
<p>where the components of the regression targets <span class="math notranslate nohighlight">\(t\)</span> are</p>
<div class="amsmath math notranslate nohighlight" id="equation-cd019e9f-c614-43dc-9f67-7b2119a39152">
<span class="eqno">(95)<a class="headerlink" href="#equation-cd019e9f-c614-43dc-9f67-7b2119a39152" title="Permalink to this equation">¶</a></span>\[\begin{eqnarray}
t_x &amp; = &amp; (G_x - P_x) / P_w \\
t_y &amp; = &amp; (G_y - P_y) / P_h \\
t_w &amp; = &amp; \log(G_w / P_w) \\
t_h &amp; = &amp; \log(G_h / P_h). \\
\end{eqnarray}\]</div>
</div>
<div class="section" id="r-cnn-performance-and-drawbacks">
<h3>R-CNN Performance and Drawbacks<a class="headerlink" href="#r-cnn-performance-and-drawbacks" title="Permalink to this headline">¶</a></h3>
<p>In <a class="bibtex reference internal" href="../referenceSection.html#girshickdonahuedarrelletal" id="id3">[GDDM14]</a> the following R-CNN performance figures were obtained:</p>
<ul class="simple">
<li><p>mAP (mean Average Precision) of <span class="math notranslate nohighlight">\(53.7\%\)</span> on PASCAL VOC 2010- compared to <span class="math notranslate nohighlight">\(35.1\%\)</span> of an approach, which uses the same region proposals, but spatial pyramid matching + BoW</p></li>
<li><p>mAP of <span class="math notranslate nohighlight">\(33.4\%\)</span> on ILSVRC 2013 Detection benchmark- compared to <span class="math notranslate nohighlight">\(24.3\%\)</span> of the previous best result OverFeat <a class="bibtex reference internal" href="../referenceSection.html#sermaneteigenzhangetal" id="id4">[SEZ+]</a></p></li>
</ul>
<figure align="center"><img src="https://maucher.home.hdm-stuttgart.de/Pics/rcnnPerformanceVOC.png" style="width:600px" align="center">
	<figcaption>
		Image Source: Source: <a href="https://arxiv.org/pdf/1311.2524.pdf">https://arxiv.org/pdf/1311.2524.pdf</a>
	</figcaption>
</figure> 
<p>The <strong>drawbacks</strong> of R-CNN are:</p>
<ul class="simple">
<li><p>3 models must be trained</p></li>
<li><p>Long training time (see picture below)</p></li>
<li><p>About 2000 region proposals must by classified per image</p></li>
<li><p>Selective Search for generating the region proposals is decoupled from R-CNN. No joint-fine tuning possible</p></li>
<li><p>Inference time has been 47 seconds per image. Not suitable for real-time applications</p></li>
<li><p>In particular for small regions the warping to a fixed size CNN-input is a \alert{waste} of computational resources and yields slow detection</p></li>
</ul>
<figure align="center"><img src="https://maucher.home.hdm-stuttgart.de/Pics/rcnnTime.png" style="width:500px" align="center">
	<figcaption>
		Image Source: Source: <a href="https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e">https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e</a>
	</figcaption>
</figure> 
</div>
</div>
<div class="section" id="spatial-pyramid-pooling-sppnet">
<h2>Spatial Pyramid Pooling (SPPnet)<a class="headerlink" href="#spatial-pyramid-pooling-sppnet" title="Permalink to this headline">¶</a></h2>
<p>Spatial Pyramid Pooling has been published 2015 in <a class="bibtex reference internal" href="../referenceSection.html#hezhangrenetal2014" id="id5">[HZRS14]</a>. It integrates <strong>Spatial Pyramid Pooling</strong> (<a class="bibtex reference internal" href="../referenceSection.html#lazebnik06" id="id6">[LSP06]</a>) into CNNs. SPPnet is applicable for classification and detection: In the ILSVRC 2014 challenge it achieved rank 3 and rank 2 in in the classification- and detection task respectively. Same as R-CNN, it is also based on region proposals**. However,</p>
<ul class="simple">
<li><p>no warping to fixed CNN-input size is required.</p></li>
<li><p>it <strong>passes each image only once through conv- and pool-layers of CNN</strong>.</p></li>
</ul>
<p>SPPnet is <span class="math notranslate nohighlight">\(24-102\)</span> times faster than R-CNN in the inference phase.</p>
<div class="section" id="problem-of-fixed-size-input-to-cnn">
<h3>Problem of fixed size input to CNN<a class="headerlink" href="#problem-of-fixed-size-input-to-cnn" title="Permalink to this headline">¶</a></h3>
<figure align="center"><img src="https://maucher.home.hdm-stuttgart.de/Pics/CNNpartitioned.png" style="width:500px" align="center">
</figure> 
<p>In a CNN, as depicted above, convolutional- and pooling layer can easily cope with varying input-size. However, <strong>fully connected layers require fixed-size input</strong>, because for this type varying size means varying number of weights.</p>
</div>
<div class="section" id="sppnet-allows-variable-size-input">
<h3>SPPnet allows variable size input<a class="headerlink" href="#sppnet-allows-variable-size-input" title="Permalink to this headline">¶</a></h3>
<p>Maybe, the most important contribution of SPPnet is it’s integration of a method, which is able to map an arbitray size output of the feature-extractor part to a fixed size input to the classifier part of a CNN.</p>
<figure align="center"><img src="https://maucher.home.hdm-stuttgart.de/Pics/sppNetFixedSize.PNG" style="width:450px" align="center">
	<figcaption>
		Image Source: Source: <a href="https://arxiv.org/pdf/1406.4729v4.pdf">https://arxiv.org/pdf/1406.4729v4.pdf</a>
	</figcaption>
</figure> 
<p>Actually this mapping method to a fixed size classifier-input is the spatial pyramid pooling method as introduced long before in <a class="bibtex reference internal" href="../referenceSection.html#lazebnik06" id="id7">[LSP06]</a>) - see also <a class="reference internal" href="../recognition/objectrecognition.html"><span class="doc std std-doc">section object recognition</span></a>. As sketched below, the arbitrary-sized feature maps at the output of the last convolutional layer are paritioned with an increasing granularity:</p>
<ul class="simple">
<li><p>on level 0 each feature map constitutes a single region</p></li>
<li><p>on level 1 each feature map is partitioned into <span class="math notranslate nohighlight">\(2 \times 2\)</span> subregions.</p></li>
<li><p>on level 2 each feature map is partitioned into <span class="math notranslate nohighlight">\(3 \times 3\)</span> subregions.</p></li>
<li><p>on level 3 each feature map is partitioned into <span class="math notranslate nohighlight">\(6 \times 6\)</span> subregions.</p></li>
</ul>
<p>Within each region max-pooling is applied to compute one value per region. The number of all regions in the entire pyramid is therefore <span class="math notranslate nohighlight">\(1+4+9+36=50\)</span>.
Since the number of all regions in the entire pyramid is independent of the size of the feature maps, the successive fully-connected layers always receive the same number of inputs. SPPnet, as defined in <a class="bibtex reference internal" href="../referenceSection.html#hezhangrenetal2014" id="id8">[HZRS14]</a> provides 256 feature maps at the last convolution layer. Therefore the fixed-size input to the classifier has length <span class="math notranslate nohighlight">\(50*256 = 12800\)</span>. As in R-CNN</p>
<ul class="simple">
<li><p>a binary linear SVM is trained for each class (now the size of the SVM input-vectors is 12800)</p></li>
<li><p>a bounding box regression model is learned.</p></li>
</ul>
<figure align="center"><img src="https://maucher.home.hdm-stuttgart.de/Pics/sppNet.png" style="width:600px" align="center">
	<figcaption>
		Image Source: Source: <a href="https://arxiv.org/pdf/1406.4729v4.pdf">https://arxiv.org/pdf/1406.4729v4.pdf</a>
	</figcaption>
</figure> 
<p>Note that with this approach, it is possible to pass the entire image only once through the CNN. Then the representation of each region proposal in the feature maps of the last conv-layer can be determined and the 256 feature maps are cropped to the size of the current region-proposal representation in this layer. Spatial pyramid max pooling, as described above, is then applied to the region proposal representation of the feature maps.</p>
<p>Compared to R-CNN, SPPnet achieves a slightly better mAP, whilst beeing much faster. The drawbacks of SPPnet are, that still 3 models must be trained. Moreover, the convolutional layers that precede the spatial pyramid pooling can not be fine-tuned, because the gradients of the error function can not be passed efficiently through the Spatial Pyramid Pooling). I.e. <strong>End-to-End training is not possible</strong>.</p>
</div>
</div>
<div class="section" id="fast-r-cnn">
<h2>Fast R-CNN<a class="headerlink" href="#fast-r-cnn" title="Permalink to this headline">¶</a></h2>
<p>Fast R-CNN has been published 2015 in <a class="bibtex reference internal" href="../referenceSection.html#girshickfastrcnn" id="id9">[Gir15]</a>. It constitutes an modification and extension of R-CNN, which is much faster in training and inference. Moreover, it achieves a higher detection quality (mAP) than R-CNN and SPPnet. It also applies region proposals (RoIs), but the image must be passed only once through the CNN. Another important improvement is that only one model must be trained using task-specific loss-functions.
Fast R-CNN also requires less memory, because there is no need to store for about 2000 RoIs per image a corresponding feature vector of length 4096.</p>
<div class="section" id="inference-process-overview">
<h3>Inference Process Overview<a class="headerlink" href="#inference-process-overview" title="Permalink to this headline">¶</a></h3>
<figure align="center"><img src="https://maucher.home.hdm-stuttgart.de/Pics/fastRCNN.PNG" style="width:500px" align="center">
	<figcaption>
		Image Source: Source: <a href="https://arxiv.org/pdf/1504.08083.pdf">https://arxiv.org/pdf/1504.08083.pdf</a>
	</figcaption>
</figure> 
</div>
<div class="section" id="roi-pooling-layer">
<h3>RoI Pooling Layer<a class="headerlink" href="#roi-pooling-layer" title="Permalink to this headline">¶</a></h3>
<p>A RoI of size <span class="math notranslate nohighlight">\(h \times w\)</span> at the last convolutional layer is partitioned into a <span class="math notranslate nohighlight">\(H \times W\)</span>-grid, where each region is approximately of size <span class="math notranslate nohighlight">\(h/H \times w/W\)</span>. Typical values: <span class="math notranslate nohighlight">\(W=H=7\)</span>. This is like level-1 partitioning in SPP. The features in each region are <strong>max-pooled</strong>, independently for each feature map.</p>
<figure align="center"><img src="https://maucher.home.hdm-stuttgart.de/Pics/roiPoolingCombined.png" style="width:550px" align="center">
	<figcaption>
		RoI Partitioning of a (5x7)-feature map into a (2x2)-grid and ROI-pooling. Image Source: Source: <a href="https://deepsense.ai/region-of-interest-pooling-explained/">https://deepsense.ai/region-of-interest-pooling-explained/</a>
	</figcaption>
</figure> 
</div>
<div class="section" id="id10">
<h3>Training<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<div class="section" id="apply-pretrained-cnns">
<h4>Apply pretrained CNNs<a class="headerlink" href="#apply-pretrained-cnns" title="Permalink to this headline">¶</a></h4>
<p>In <a class="bibtex reference internal" href="../referenceSection.html#girshickfastrcnn" id="id11">[Gir15]</a> three CNNs of different size are poposed. No matter which of them is applied in it’s pretrained version, three transformation steps are required to integrate it in Fast R-CNN:</p>
<ol class="simple">
<li><p>the last max pooling layer is replaced by a RoI pooling layer that is configured by setting <span class="math notranslate nohighlight">\(H\)</span> and <span class="math notranslate nohighlight">\(W\)</span> to be compatible with the net’s first fully connected layer.</p></li>
<li><p>the network’s last fully connected layer (which was trained for 1000 objects) is replaced with the two sibling layers: a fully connected layer for <span class="math notranslate nohighlight">\(K + 1\)</span> categories and a category-specific bounding-box regressors.</p></li>
<li><p>the network is modified to take two data inputs: a list of images and a list of RoIs in those images.</p></li>
</ol>
</div>
<div class="section" id="hierarchical-sampling-enables-fine-tuning">
<h4>Hierarchical Sampling enables Fine-Tuning<a class="headerlink" href="#hierarchical-sampling-enables-fine-tuning" title="Permalink to this headline">¶</a></h4>
<p>In R-CNN and SPPnet minibatches of size <span class="math notranslate nohighlight">\(N=128\)</span> were constructed by <strong>sampling one RoI from 128 different images</strong>. Sampling multiple RoIs from a single image was supposed to be inadequate, because RoIs from the same image are correlated, causing slow training convergence. Since at the end of the feature extractor each RoI has a large receptive field (sometimes covering the entire image), fine-tuning of the Feature Extractor would be very expensive, if each element of the batch comes from a different image. Therefore, <strong>the Feature Extractor has not been fine-tuned in R-CNN and SPPnet</strong>.</p>
<p>In Fast-RCNN minibatches are sampled hierarchically:</p>
<ol class="simple">
<li><p>sample <span class="math notranslate nohighlight">\(N\)</span> images (<span class="math notranslate nohighlight">\(N=2\)</span>),</p></li>
<li><p>sample <span class="math notranslate nohighlight">\(R/N\)</span> RoIs from each image (<span class="math notranslate nohighlight">\(R=128\)</span>).</p></li>
</ol>
<p><strong>Samples from the same image share computation and memory in Forward- and Backwardpass</strong>. This yields a <span class="math notranslate nohighlight">\(64 \times \)</span> decrease in fine-tuning time compared to sampling <span class="math notranslate nohighlight">\(R=128\)</span> different images. This is why <strong>fine-tuning the feature extractor is possible in Fast R-CNN</strong>. The concern of slow convergence due to correlated samples within a minibatch has appeared to be not true in the research on Fast R-CNN.</p>
</div>
<div class="section" id="multi-task-loss">
<h4>Multi-Task Loss<a class="headerlink" href="#multi-task-loss" title="Permalink to this headline">¶</a></h4>
<p>Fast R-CNN has <strong>Two output-layers</strong>:</p>
<ul class="simple">
<li><p>Softmax-Output with <span class="math notranslate nohighlight">\(K+1\)</span> neurons for class-probabilities <span class="math notranslate nohighlight">\(p=(p_0,p_1,\ldots,p_K)\)</span></p></li>
<li><p>Bounding-Box regressors <span class="math notranslate nohighlight">\(t^k=(t^k_x,t^k_y,t^k_w,t^k_h)\)</span> for each of the <span class="math notranslate nohighlight">\(K\)</span> classes</p></li>
</ul>
<p>Each training RoI is labeled with a groundtruth class <span class="math notranslate nohighlight">\(u\)</span> and a groundtruth bounding-box regresssion target <span class="math notranslate nohighlight">\(v\)</span>. The multi-task loss <span class="math notranslate nohighlight">\(L\)</span> on each labeled RoI for <strong>jointly training classification and bounding-box regression</strong>:</p>
<div class="math notranslate nohighlight">
\[
L(p,u,t^u,v)=L_{cls} +\lambda [u \geq 1] L_{loc}(t^u,v),
\]</div>
<p>where</p>
<ul class="simple">
<li><p>the <span class="math notranslate nohighlight">\([u \geq 1]\)</span>-operator evaluates to 1 for <span class="math notranslate nohighlight">\(u \geq 1\)</span> and <span class="math notranslate nohighlight">\(0\)</span> otherwise (class-index <span class="math notranslate nohighlight">\(u=0\)</span> indicates <em>no-known class</em>).</p></li>
<li><p><span class="math notranslate nohighlight">\(L_{cls}\)</span> is the log-loss function for true class <span class="math notranslate nohighlight">\(u\)</span> and <span class="math notranslate nohighlight">\(L_{loc}\)</span> is a <span class="math notranslate nohighlight">\(L_1\)</span> loss-function between the elements of the predicted- and the groundtruth bounding-box quadruple (see <a class="bibtex reference internal" href="../referenceSection.html#girshickfastrcnn" id="id12">[Gir15]</a>).</p></li>
</ul>
</div>
</div>
</div>
<div class="section" id="faster-r-cnn">
<h2>Faster R-CNN<a class="headerlink" href="#faster-r-cnn" title="Permalink to this headline">¶</a></h2>
<p>Faster R-CNN has been published 2015 in <a class="bibtex reference internal" href="../referenceSection.html#girshickfaster" id="id13">[RHGS15]</a>. In R-CNN and Fast R-CNN <a class="reference external" href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf">Selective Search</a>) has been applied for generating the Region Proposals (RoIs). Faster R-CNN is based on R-CNN and Fast R-CNN but replaces <em>Selective Search</em> by a <strong>Region Proposal Network (RPN)</strong>, that shares full-image convolutional features with the detection network. An RPN is a <strong>fully-convolutional network</strong> that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high- quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features.</p>
<figure align="center"><img src="https://maucher.home.hdm-stuttgart.de/Pics/fasterRCNNoverview.png" style="width:400px" align="center">
	<figcaption>
		Region Proposal Network (RPN) in Faster R-CNN. The convolutional layers at the bottom are shared by the RPN and the Fast R-CNN module.
	</figcaption>
</figure> 
<p>Faster-RCNNs overall process can be summarized as follows:</p>
<ol class="simple">
<li><p>RPN generates region proposals, based on the last feature map of the extractor CNN</p></li>
<li><p>For all region proposals in the image, a fixed-length feature vector is extracted from each region using the ROI Pooling layer.</p></li>
<li><p>The extracted feature vectors are classified using the Fast R-CNN.</p></li>
<li><p>The extracted feature vectors are used to adjust the bounding-boxes</p></li>
<li><p>The class scores of the detected objects in addition to their bounding-boxes are returned.</p></li>
</ol>
<figure align="center"><img src="https://maucher.home.hdm-stuttgart.de/Pics/fasterRCNNanchors.png" style="width:600px" align="center">
	<figcaption>
		Each position in the feature maps of the final conv-layer is a anchor. Each anchor has it's correspondence in the input-image. Around each anchor in the input-image a set of 9 anchor-boxes is defined.
	</figcaption>
</figure> 
<div class="section" id="rpn">
<h3>RPN<a class="headerlink" href="#rpn" title="Permalink to this headline">¶</a></h3>
<figure align="center"><img src="https://maucher.home.hdm-stuttgart.de/Pics/fasterRCNNrpn.png" style="width:600px" align="center">
	<figcaption>
		Region Proposal Network is a fully convolutional network, which calculates for each anchor-box an objectiveness-score and a bounding-box-adjustment.
</figure> 
<p>For each of the <span class="math notranslate nohighlight">\(14 \times 14 = 196\)</span> anchors in the input to the RPN, there exists <span class="math notranslate nohighlight">\(k=9\)</span> different anchor-boxes. Each anchor box is defined by its corresponding coordinates in the image <span class="math notranslate nohighlight">\((x_c,y_c,w,h)\)</span>. To each anchor box in the training images a <strong>ground-truth-label</strong> is assigned as follows:</p>
<ul class="simple">
<li><p>The label of the anchor-box is <span class="math notranslate nohighlight">\(fg\)</span> (foreground), if the the IoU of the anchor-box and a ground-truth bounding-box object bounding box is <span class="math notranslate nohighlight">\(\geq 0.5\)</span></p></li>
<li><p>If the IoU is <span class="math notranslate nohighlight">\(&lt;0.1\)</span> the label of the anchor-box is <span class="math notranslate nohighlight">\(bg\)</span> (background)</p></li>
</ul>
<p>The RPN uses all the anchor-boxes selected for the minibatch of size 256 to calculate the objectness-score (foreground/background) and the corresponding classification loss using <strong>binary cross entropy</strong>. Then, it uses only those minibatch anchors marked as foreground to calculate the regression loss. For calculating the targets for the regression, the foreground anchor is compared with the closest ground-truth object in order to calculate the correct adjustment <span class="math notranslate nohighlight">\((\Delta_{x_c},\Delta_{y_c},\Delta_{w},\Delta_{h})\)</span>.
For measuring the regression error, the <a class="bibtex reference internal" href="../referenceSection.html#girshickfaster" id="id14">[RHGS15]</a> suggests <strong>smooth L1 loss</strong>. If</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\((\Delta_{x_c},\Delta_{y_c},\Delta_{w},\Delta_{h})\)</span> is the correct adjustment</p></li>
<li><p><span class="math notranslate nohighlight">\((\delta_{x_c},\delta_{y_c},\delta_{w},\delta_{h})\)</span> is the predicted adjustment</p></li>
</ul>
<p>then the contribution of this single bounding-box to the smoothed L1-loss is defined to be</p>
<div class="math notranslate nohighlight">
\[
L_{loc}(\Delta,\delta) = \sum\limits_{i \in \{x_c,y_c,w,h\}} smooth_{L1}(\Delta_i - \delta_i)
\]</div>
<p>with</p>
<div class="math notranslate nohighlight">
\[\begin{split}
smooth_{L1} (x) = \left\{
\begin{array}{cc}
0.5 \cdot x^2 &amp; \mbox{ if } |x| &lt; \sigma \\
|x|-0.5 &amp; \mbox{ else, } 
\end{array}
\right.
\end{split}\]</div>
<p>Smooth L1 is basically L1, but when the L1 error is small, defined by a certain <span class="math notranslate nohighlight">\(\sigma &lt; 1\)</span>, the error is considered almost correct and the loss diminishes at a faster rate.</p>
<p><strong>Non-maximum suppression:</strong></p>
<p>Since anchors usually overlap, proposals which belong to the same object may also overlap. To solve this problem Non-Maximum Suppression (NMS) is applied. NMS takes the list of proposals sorted by score and iterates over the sorted list, discarding those proposals that have an IoU larger than some predefined threshold (e.g <span class="math notranslate nohighlight">\(IoU &gt; 0.6\)</span>) with a proposal that has a higher score. After applying NMS, the top <span class="math notranslate nohighlight">\(N\)</span> proposals, sorted by score, are kept, while the others are disregarded (<span class="math notranslate nohighlight">\(N=2000\)</span> in <a class="bibtex reference internal" href="../referenceSection.html#girshickfaster" id="id15">[RHGS15]</a>).</p>
<p>Note that RPN can be used stand-alone, without needing the second stage model, in problems where there is only a single class of objects, the objectness probability can be used as the final class probability.</p>
</div>
<div class="section" id="roi-pooling-and-r-cnn">
<h3>RoI-Pooling and R-CNN<a class="headerlink" href="#roi-pooling-and-r-cnn" title="Permalink to this headline">¶</a></h3>
<p>Once, the region proposals are calculated by the RPN, they are passed to a RoI-Pooling such as in Fast-RCNN. The constant-length feature vector, which is provided by RoI-pooling is then past to the R-CNN, which consists of a classifier and a bounding-box regression.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./deeplearning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="cnns.html" title="previous page">Convolutional Neural Networks for Object Recognition</a>
    <a class='right-next' id="next-link" href="../face/faceDetection.html" title="next page">Face Detection</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Prof. Dr. Johannes Maucher<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>