
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Intro and Overview Object Recognition Lecture &#8212; Object Recognition Lecture</title>
    
  <link rel="stylesheet" href="_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Basic Image Access Operations" href="preprocessing/01accessImage.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="index.html">
  
  <img src="_static/hdmlogomed.jpg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Object Recognition Lecture</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1 current active">
  <a class="reference internal" href="#">
   Intro and Overview Object Recognition Lecture
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Image Processing
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="preprocessing/01accessImage.html">
   Basic Image Access Operations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="preprocessing/02filtering.html">
   Basic Filter Operations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="preprocessing/04gaussianDerivatives.html">
   Gaussian Filter and Derivatives of Gaussian
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="preprocessing/03LowPassFilter.html">
   Rectangular- and Gaussian Low Pass Filtering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="preprocessing/06GaussianNoiseReduction.html">
   Noise Suppression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="preprocessing/05GaussianLowPassFilter.html">
   Gaussian and Difference of Gaussian Pyramid
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Features
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="features/globalDescriptors.html">
   Global Image Features
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="features/similarityMetrics.html">
   Similarity Measures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="features/ImageRetrieval.html">
   Histogram-based Image Retrieval
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="features/ImageRetrieval.html#use-pretrained-cnns-for-retrieval">
   Use pretrained CNNs for Retrieval
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="features/multiReceptiveFields.html">
   Multidimensional Receptive Field Histograms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="features/naiveBayesHistogram.html">
   Histogram-based Naive Bayes Recognition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="features/probRecognition.html">
   Example: Naive Bayes Object Recognition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="features/localFeatures.html">
   Local Image Features
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="features/harrisCornerDetection.html">
   Example: Harris-FÃ¶rstner Corner Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="features/siftDescriptorCV2.html">
   Example: Create SIFT Descriptors with openCV
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="features/HoGfeatures.html">
   Histogram of Oriented Gradients: Step-by-Step
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="features/HOGpedestrianDetection.html">
   HOG-based Pedestrian Detection
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Object Recognition
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="recognition/objectrecognition.html">
   Object Recognition
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Face Detection and Recognition
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="face/faceDetection.html">
   Face Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="face/faceRecognition.html">
   Face Recognition using FaceNet
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Pose Estimation
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="poseEstimation/Pose_Estimation.html">
   Multi-Person 2D Pose Estimation using Part Affinity Fields
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="referenceSection.html">
   References
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/intro.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-object-recognition">
   What is Object Recognition?
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#part-1-basic-image-processing">
     Part 1: Basic Image Processing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#part-2-feature-extraction">
     Part 2: Feature Extraction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#part-3-machine-learning-algorithm-for-object-recognition">
     Part 3: Machine Learning Algorithm for Object Recognition
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#why-object-recognition-is-challenging">
   Why Object Recognition is challenging?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#some-applications">
   Some Applications
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#google-vision-api">
     Google Vision API
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#image-retrieval">
     Image Retrieval
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#image-based-disease-detection">
     Image based Disease Detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#forgery-detection">
     Forgery Detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#optical-inspection">
     Optical Inspection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#autonomous-driving">
     Autonomous Driving
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tracking">
     Tracking
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pose-estimation">
     Pose Estimation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#style-transfer">
     Style Transfer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#super-resolution">
     Super Resolution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#image-generation">
     Image Generation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generating-images-from-text">
     Generating Images from Text
    </a>
    <ul class="nav section-nav flex-column">
    </ul>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="intro-and-overview-object-recognition-lecture">
<h1>Intro and Overview Object Recognition Lecture<a class="headerlink" href="#intro-and-overview-object-recognition-lecture" title="Permalink to this headline">Â¶</a></h1>
<div class="section" id="what-is-object-recognition">
<h2>What is Object Recognition?<a class="headerlink" href="#what-is-object-recognition" title="Permalink to this headline">Â¶</a></h2>
<div class="admonition-goal-of-computer-vision admonition">
<p class="admonition-title">Goal of Computer Vision</p>
<p>Computer Vision seeks to enable machines to see and understand data from images and videos</p>
</div>
<div class="admonition-definition-of-computer-vision admonition">
<p class="admonition-title">Definition of Computer Vision</p>
<p>A branch of artificial intelligence and image processing
concerned with computer processing of images from the real
world.  Computer vision typically requires a combination of
low level image processing to enhance the image quality
(e.g. remove noise, increase contrast) and higher level
pattern recognition and image understanding to recognise
features present in the image.</p>
</div>
<p>Object Recognition is the central task within Computer Vision. Other Computer-Vision tasks are e.g. <em>image restoration</em>, <em>3D-reconstruction</em>, <em>image rendering</em> etc. Computer Vision itself is related with other sciences, as depicted in the image below:</p>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/relatedScience.png" style="width:500px" align="center">
<figcaption>
</figcaption>
</figure>
<p>Object recognition can be considered to be a subset of Machine Learning. The general process of Machine Learning (ML) consists of a training- and an inference-phase:</p>
<ol>
<li><p>During the <strong>training-phase</strong> a ML-algorithm learns from a large set of labeled input (pairs of <span class="math notranslate nohighlight">\((input/output)\)</span>) a general model, which can be considered to be a function <span class="math notranslate nohighlight">\(output=f(input)\)</span>.</p>
 <figure align="center">
 <img src="https://maucher.home.hdm-stuttgart.de/Pics/introExampleLearning.png" style="width:500px" align="center">
 <figcaption>
      Supervised learning training-phase: Learn an abstract model from many pairs of labeled input
 </figcaption>
 </figure>
</li>
<li><p><strong>Inference Phase:</strong> The model <span class="math notranslate nohighlight">\(output=f(input)\)</span>, which has been learned in the training-phase, is applied to calcultate for each new <span class="math notranslate nohighlight">\(input\)</span> the corresponding <span class="math notranslate nohighlight">\(output\)</span>.</p></li>
</ol>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/introExampleLearningApply.png" style="width:500px" align="center">
<figcaption>
     Inference Phase: Apply learned model to classify new input
</figcaption>
</figure>
<p>On a more technical level the concept of supervised Machine-Learning can be sketched as follows:</p>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/SupervisedLearningSchemaEnglish.png" style="width:500px" align="center">
<figcaption>
</figcaption>
</figure>
<p>As shown in the figure above, the input usually must be transformed into a numeric representation (vector or multi-dimensional array), which must somehow contain the relevant information of the input.</p>
<p>In the case of Object Recognition, the <span class="math notranslate nohighlight">\(input\)</span> to the ML-process is either an image or a video and the <span class="math notranslate nohighlight">\(output\)</span> are the detected objects.</p>
<p>In object recognition the <em>preprocessing block</em>, is particularly important. There exist many different approaches to exract relevant features from images and videos and to encode these features into numeric representations. The performance and quality of an object recognition process strongly depends on the <strong>quality of the images</strong>, the <strong>type of feature extraction</strong> and the performance and suitability of the *<em>applied Machine Learning algorithm</em>.</p>
<p>Hence, this lecture can be partitioned into the subfields:</p>
<ol class="simple">
<li><p>Basic Image Processing</p></li>
<li><p>Feature Extraction and Representation</p></li>
<li><p>Machine Learning Algorithms</p></li>
</ol>
<div class="section" id="part-1-basic-image-processing">
<h3>Part 1: Basic Image Processing<a class="headerlink" href="#part-1-basic-image-processing" title="Permalink to this headline">Â¶</a></h3>
<p>The first block on basic image processing starts <a class="reference internal" href="preprocessing/02filtering.html"><span class="doc std std-doc">here</span></a>. Image processing is performed by applying filters to images. The most important type of filtering - convolutional filtering - will be introduced in detail. In object recognition filters are applied e.g. for</p>
<ul class="simple">
<li><p>enhancing the quality of images, e.g. by noise-suppression or contrast-enhancement</p></li>
<li><p>generating different versions, e.g. different scales or blurs, of an image</p></li>
<li><p>detecting features such as edges</p></li>
<li><p>keypoint-detection</p></li>
<li><p>template-matching</p></li>
</ul>
</div>
<div class="section" id="part-2-feature-extraction">
<h3>Part 2: Feature Extraction<a class="headerlink" href="#part-2-feature-extraction" title="Permalink to this headline">Â¶</a></h3>
<p>The block on feature extraction starts <a class="reference internal" href="features/globalDescriptors.html"><span class="doc std std-doc">here</span></a>. For object recognition and other computer-vision tasks images must be described by one or more numeric representations. These numeric representations should contain the relevant <strong>features</strong> of the image. On an abstract level, the methods to calculate features from a given image can be categorized as follows:</p>
<ul class="simple">
<li><p><strong>Global features:</strong> One numeric representation (vector) is calculated for the entire image. Further recognition/identification applies this single image descriptor as input.</p></li>
</ul>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/strawberryIntensities.PNG" style="width:500px" align="center">
<figcaption>
    Example for an global descriptor: For all pixels and all channels the intensity-values are arranged in a numeric vector of length $channels x height x width$. 
</figcaption>
</figure>
<ul class="simple">
<li><p><strong>Subwindow features:</strong> The entire image is partitioned into subwindows. For each subwindow one numeric representation is calculated and recognition/identification is performed for each single subwindow.</p></li>
</ul>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/slidingWindow.jpg" style="width:500px" align="center">
<figcaption>
    Example: Sliding Window Object Detection 
</figcaption>
</figure>
<ul class="simple">
<li><p><strong>Subspace features:</strong> The feature descriptor of the entire image or a subwindow can be transformed into a lower-dimensional space, e.g. by Principal Component Analysis (PCA), Singular Value Decomposition (SVD) or an Autoencoder. Recognition may perform better in the lower-dimensional space, than in the original space. Face recognition is often implemented on the basis of subspace features.</p></li>
</ul>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/faceRecognitionTraining.PNG" style="width:500px" align="center">
<figcaption>
    Example Application for Subspace-Features: Face recognition 
</figcaption>
</figure>
<ul class="simple">
<li><p><strong>Local features:</strong> A set of numeric representations (vectors) is calculated for a given image. Each of these vectors describes a local area within the image. Further recognition/identification is performed by taking into account the entire set of local descriptors.</p></li>
</ul>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/siftRecognLorry.PNG" style="width:500px" align="center">
<figcaption>
    Example Local Features: A large set of descriptors, each describing a local area (keypoint), is extracted from the image. Sets of local descriptors are applied to recognize objects.   
</figcaption>
</figure>
</div>
<div class="section" id="part-3-machine-learning-algorithm-for-object-recognition">
<h3>Part 3: Machine Learning Algorithm for Object Recognition<a class="headerlink" href="#part-3-machine-learning-algorithm-for-object-recognition" title="Permalink to this headline">Â¶</a></h3>
<p>The general notion <em>Object Recognition</em>, can actually be subdivided into different dimensions and subtasks. Here we distinguish</p>
<ul class="simple">
<li><p><strong>General Object recognition:</strong> Determine which object-category, e.g. car, person, building etc. is in the image</p></li>
<li><p><strong>Identification:</strong> Determine which concrete instance is in the image, e.g. which concrete person</p></li>
</ul>
<p>A further distinction is shown in the image below:</p>
<img alt="https://maucher.home.hdm-stuttgart.de/Pics/objectRecognitionTasks.png" class="align-center" src="https://maucher.home.hdm-stuttgart.de/Pics/objectRecognitionTasks.png" />
<p>Image Source: <a class="reference external" href="https://medium.com/&#64;nikasa1889/the-modern-history-of-object-recognition-infographic-aea18517c318">Blog Post: The Modern History of Object Recognition â Infographic</a> and <a class="reference external" href="https://github.com/Nikasa1889/HistoryObjectRecognition">the corresponding Github repo</a></p>
<p>Today, most object recognition tasks can be solved best by <strong>Deep Nerual Networks</strong>. However, Deep Learning requires usually a large amount of labeled training data and the availability of sufficient computing power. Moreover, some tasks can still be solved better by applying conventional ML-approaches. In this lecture deep-learning as well as conventional ML-approaches are described.</p>
</div>
</div>
<div class="section" id="why-object-recognition-is-challenging">
<h2>Why Object Recognition is challenging?<a class="headerlink" href="#why-object-recognition-is-challenging" title="Permalink to this headline">Â¶</a></h2>
<blockquote class="epigraph">
<div><p>The fact that about half of the cerebral cortex
in primates is devoted to processing visual information gives some indication of the computational load one can expect to invest for this complex task.</p>
</div></blockquote>
<p>Source: <a class="bibtex reference internal" href="referenceSection.html#felleman" id="id1">[FvE91]</a></p>
<p>Particularly in the case of</p>
<ul class="simple">
<li><p><strong>Image Retrieval</strong> with large image databases,</p></li>
<li><p><strong>Image Recognition</strong>, where an image must be categorized in one of thousands of categories</p></li>
</ul>
<p>highly efficient algorithms are required.</p>
<p>In the case of supervised learning <strong>labeled training data</strong> is required. In general labeling is cost-intensive.</p>
<p>Object recognition must be robust with respect to scale, illumination, translation and rotation, viewpoint, clutter, occlusion, pose, intra-class invariance:</p>
<img alt="https://maucher.home.hdm-stuttgart.de/Pics/kuehe3.jpg" class="align-center" src="https://maucher.home.hdm-stuttgart.de/Pics/kuehe3.jpg" />
</div>
<div class="section" id="some-applications">
<h2>Some Applications<a class="headerlink" href="#some-applications" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="google-vision-api">
<h3>Google Vision API<a class="headerlink" href="#google-vision-api" title="Permalink to this headline">Â¶</a></h3>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/googleVisionAPI.png" style="width:500px" align="center">
<figcaption>
    Example: Detect and localize all objects in an image 
</figcaption>
</figure>
</div>
<div class="section" id="image-retrieval">
<h3>Image Retrieval<a class="headerlink" href="#image-retrieval" title="Permalink to this headline">Â¶</a></h3>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/googlePhotos.png" style="width:500px" align="center">
<figcaption>
    Example Image Retrieval: <a href = "https://images.google.com">https://images.google.com</a>  
</figcaption>
</figure>
</div>
<div class="section" id="image-based-disease-detection">
<h3>Image based Disease Detection<a class="headerlink" href="#image-based-disease-detection" title="Permalink to this headline">Â¶</a></h3>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/lungCovid.png" style="width:500px" align="center">
<figcaption>
    Example: Distinguish benign from malignant cases in radiology-images  
</figcaption>
</figure>
</div>
<div class="section" id="forgery-detection">
<h3>Forgery Detection<a class="headerlink" href="#forgery-detection" title="Permalink to this headline">Â¶</a></h3>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/britishPassport.jpg" style="width:500px" align="center">
<figcaption>
    Example: Detect counterfeit identification documents based on scanned images. See also <a href = "https://ai.hdm-stuttgart.de/research/dokiq/">DOKIQ Research Project</a>   
</figcaption>
</figure>
</div>
<div class="section" id="optical-inspection">
<h3>Optical Inspection<a class="headerlink" href="#optical-inspection" title="Permalink to this headline">Â¶</a></h3>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/opticalInspectionExamples.png" style="width:500px" align="center">
<figcaption>
    Example: Detect failures based on surface images 
</figcaption>
</figure>
</div>
<div class="section" id="autonomous-driving">
<h3>Autonomous Driving<a class="headerlink" href="#autonomous-driving" title="Permalink to this headline">Â¶</a></h3>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/cityscape.png" style="width:500px" align="center">
<figcaption>
    Example: <a href = "https://www.cityscapes-dataset.com">Cityscape Dataset for semantic segmentation</a>  
</figcaption>
</figure>
</div>
<div class="section" id="tracking">
<h3>Tracking<a class="headerlink" href="#tracking" title="Permalink to this headline">Â¶</a></h3>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/yoloLocalisationRun.png" style="width:500px" align="center">
<figcaption>
    Example: Localize and track objects in video 
</figcaption>
</figure>
</div>
<div class="section" id="pose-estimation">
<h3>Pose Estimation<a class="headerlink" href="#pose-estimation" title="Permalink to this headline">Â¶</a></h3>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/poseEstimation.png" style="width:500px" align="center">
<figcaption>
    Example: Multiperson pose estimation and pracking in video 
</figcaption>
</figure>
</div>
<div class="section" id="style-transfer">
<h3>Style Transfer<a class="headerlink" href="#style-transfer" title="Permalink to this headline">Â¶</a></h3>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/styleTransferBoth.png" style="width:500px" align="center">
<figcaption>
    Example: Style transfer of images 
</figcaption>
</figure>
</div>
<div class="section" id="super-resolution">
<h3>Super Resolution<a class="headerlink" href="#super-resolution" title="Permalink to this headline">Â¶</a></h3>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/superresolutionExample.png" style="width:500px" align="center">
<figcaption>
    Example: GAN-based Super Resolution 
</figcaption>
</figure>
</div>
<div class="section" id="image-generation">
<h3>Image Generation<a class="headerlink" href="#image-generation" title="Permalink to this headline">Â¶</a></h3>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/gauGanDemo.png" style="width:500px" align="center">
<figcaption>
    Example: <a href = "http://nvidia-research-mingyuliu.com/gaugan/">Nvidia GauGan Demo</a>  
</figcaption>
</figure>
</div>
<div class="section" id="generating-images-from-text">
<h3>Generating Images from Text<a class="headerlink" href="#generating-images-from-text" title="Permalink to this headline">Â¶</a></h3>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/Dall-E.png" style="width:500px" align="center">
<figcaption>
    Example: <a href = "https://openai.com/blog/dall-e/">OpenAI DALL-E based on GPT-3 Transformer Language Model</a>  
</figcaption>
</figure>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='right-next' id="next-link" href="preprocessing/01accessImage.html" title="next page">Basic Image Access Operations</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Prof. Dr. Johannes Maucher<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>