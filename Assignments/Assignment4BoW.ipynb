{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Bag of Visual Words\n",
    "\n",
    "* Credits: 40\n",
    "* Submit .ipynb, .html to maucher(at)hdm-stuttgart.de\n",
    "\n",
    "## Idea\n",
    "In notebook [01ImageRetrieval.ipynb](../imageRetrieval/01ImageRetrieval.ipynb) an image retrieval task based on color-histograms has been implemented. In the current exercise this CBIR-task shall be implemented in the same way. However, now each image shall be represented by it's BoW-vector (instead of the color-histogram). The question is: Does the BoW representation provide a better accuracy on the CBIR-task, if the same classification-algorithm (1-nearest neighbor) is applied on the same benchmark-image-set ([Another 53 objects database](http://www.vision.ee.ethz.ch/datasets/index.en.html)).  \n",
    "\n",
    "In order to solve this task we have to\n",
    "\n",
    "1. Extract the set of local descriptors of each image in the given dataset [Another 53 objects database](http://www.vision.ee.ethz.ch/datasets/index.en.html).\n",
    "2. Calculate the visual vocabulary, i.e. the set of visual words\n",
    "3. Represent each image by its BoW-vector\n",
    "4. Apply 1-nearest-neighbour classification and 1-over-all testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Calculate local descriptors of all images\n",
    "As in [01ImageRetrieval.ipynb](../imageRetrieval/01ImageRetrieval.ipynb) loop over all images of the ([Another 53 objects database](http://www.vision.ee.ethz.ch/datasets/index.en.html)) benchmark-dataset and:\n",
    "\n",
    "1. import the image as greyscale-image\n",
    "2. calculate the ORB-local descriptors by applying the corresponding opencv-class ([ORB-features in opencv](https://docs.opencv.org/3.4/d1/d89/tutorial_py_orb.html)). ORB features are very similar to SIFT-features, but faster to compute. For this create an oRB-object and apply it's `detectAndCompute()`-method. For more details on ORB, see e.g. [ORB Features](https://medium.com/data-breach/introduction-to-orb-oriented-fast-and-rotated-brief-4220e8ec40cf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Calculate Visual Words\n",
    "Apply the [K-Means Algorithm from scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) for calculating $K$ visual words (K clusters) from the set of all local descriptors of all images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Calculate Bag-Of-Word Representation of each image\n",
    "For each image calculate the BoW-representation by:\n",
    "1. Applying K-Means' `predict()`-method to all local descriptors of a single image. This method returns for all local descriptors of the image the index of the cluster to which the local descriptor is assigned to.\n",
    "2. Applying [Numpy's unique()-method](https://numpy.org/doc/stable/reference/generated/numpy.unique.html) for calculating the cluster-indices and their frequency in the image. If the array, returned by the predict()-method in the previous step is denoted by $h$, then `cluster_centers, center_frequencies = np.unique(h, return_counts=True)` provides all the information for calculating the BoW-of a single image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: One-over-all testing\n",
    "Apply the same method as in notebook [01ImageRetrieval.ipynb](../imageRetrieval/01ImageRetrieval.ipynb) for one-over-all-testing (actually you may copy the corresponding code-snippet), i.e. for each image $I_q$: \n",
    "1. calculate the euclidean distances between the image's bow and the bows of all other images\n",
    "2. determine the image, whose bow is closest to the bow of $I_q$.\n",
    "3. if this closest image contains the same object as $I_q$, the recognition is correct, otherwise it is erroneous.\n",
    "4. Calculate the overall error-rate for this one-over-all test.\n",
    "5. Modify the number of visual words $K$ and determine how the error rate varies with an increasing $K$\n",
    "\n",
    "Don't be disappointed, if your best error-rate is worse than in [01ImageRetrieval.ipynb](../imageRetrieval/01ImageRetrieval.ipynb). Try to explain! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
