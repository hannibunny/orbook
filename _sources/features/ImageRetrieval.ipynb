{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histogram-based Image Retrieval\n",
    "As introduced in [Swain, Ballard](http://www.cs.utexas.edu/~dana/Swain1.pdf) ({cite}`swainballard`)  3-dimensional color histograms can be applied for specific object recognition. We consider the image retrieval case, i.e. given a large database of images and a novel query image, the task is to determine the database image which is most similar to the query image. The images in the database as well as the query image are represented by their 3-dimensional color histograms. Hence, the search for the most similar image in the database is equal to find the 3-d color histogram, which matches best to the 3-d color historgram of the query image.\n",
    "\n",
    "In this demo the [Another 53 objects database](http://www.vision.ee.ethz.ch/datasets/index.en.html) is applied. The database contains for each of the 53 objects 5 images, which differ in viewpoint, rotation vertical- and horizontal alignment of the object. Actually, the dataset contains one object, for which only one image exists. This image (image215) has been removed for the test below. A sample of the image database is shown in the picture below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/53objects.png\" style=\"width:500px\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testprocedure\n",
    "\n",
    "In order to evaluate the recognition performance of the 3-d color histogram image retrieval system a leave-one-out test procedure is applied. This means that in each of the 265 iterations one image is selected as query image and the remaining 264 images constitute the database. The recognition is considered to be correct, if the best matching image contains the same object as the query image.\n",
    "\n",
    "Recognition performance depends on the metric used for comparing multidimensional histograms. In this demo the following distance measures are compared:\n",
    "\n",
    "* Euclidean\n",
    "* Pearson Correlation\n",
    "* Canberra\n",
    "* Bray-Curtis\n",
    "* Intersection\n",
    "* Bhattacharyya\n",
    "* Chi-Square\n",
    "\n",
    "For a formal definition of these metrics see [previous section](similarityMetrics.ipynb).\n",
    "\n",
    "## Implementation\n",
    "\n",
    "The following modules are applied:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from scipy.spatial.distance import *\n",
    "from scipy.stats import wasserstein_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance functions\n",
    "\n",
    "The [Scipy package spatial.distance](http://docs.scipy.org/doc/scipy/reference/spatial.distance.html) contains methods for calculating the euclidean-, Pearson-Correlation-, Canberra- and Bray-Curtis-distance. The remaining distance measures as well as a corresponding normalization function are defined in this python program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(a):\n",
    "    s=np.sum(a)\n",
    "    return a/s\n",
    "\n",
    "def mindist(a,b):\n",
    "    return 1.0/(1+np.sum(np.minimum(a,b)))\n",
    "\n",
    "def bhattacharyya(a,b): # this is acutally the Hellinger distance,\n",
    "                        # which is a modification of Bhattacharyya\n",
    "                        # Implementation according to http://en.wikipedia.org/wiki/Bhattacharyya_distance\n",
    "    anorm=normalize(a)\n",
    "    bnorm=normalize(b)\n",
    "    BC=np.sum(np.sqrt(anorm*bnorm))\n",
    "    \n",
    "    if BC > 1:\n",
    "        #print BC\n",
    "        return 0\n",
    "    else:\n",
    "        return np.sqrt(1-BC)\n",
    "\n",
    "def chi2(a,b):\n",
    "    idx=np.nonzero(a+b)\n",
    "    af=a[idx]\n",
    "    bf=b[idx]\n",
    "    return np.sum((af-bf)**2/(af+bf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The applied metrics are defined as a list of function names and as a list of strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods=[euclidean,correlation,canberra,braycurtis,mindist,bhattacharyya,chi2,wasserstein_distance]\n",
    "methodName=[\"euclidean\",\"correlation\",\"canberra\",\"braycurtis\",\"intersection\",\"bhattacharyya\",\"chi2\",\"emd\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the number of bins per dimension B is defined. The 3-dimensional color histogram then consists of $B\\cdot B\\cdot B$ bins. The images are imported from the locally saved image database, which contains the 265 .jpeg images. For calculating the 3-dimensional color histograms of the images the Numpy method histogramdd is applied. The histograms are normalized and flattened (transformed into a 1-dimensional array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(265, 125)\n"
     ]
    }
   ],
   "source": [
    "B=5\n",
    "# create a list of images\n",
    "path = '../Data/66obj/images'\n",
    "imlist = [os.path.join(path,f) for f in os.listdir(path) if f.endswith(\".JPG\")]\n",
    "imlist.sort()\n",
    "#print(imlist)\n",
    "features = np.zeros([len(imlist), B*B*B])\n",
    "hists=[]\n",
    "for i,f in enumerate(imlist):\n",
    "    im = np.array(Image.open(f))\n",
    "    # multi-dimensional histogram\n",
    "    h=cv2.calcHist([im],[0,1,2],None,[B,B,B],[0,256,0,256,0,256])\n",
    "    h=normalize(h) #normalization\n",
    "    features[i] = h.flatten()\n",
    "    hists.append(h)\n",
    "print(features.shape)\n",
    "N_IM=features.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Correct</th>\n",
       "      <th>Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>euclidean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correlation</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canberra</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>braycurtis</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intersection</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bhattacharyya</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chi2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emd</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Correct Error\n",
       "euclidean         NaN   NaN\n",
       "correlation       NaN   NaN\n",
       "canberra          NaN   NaN\n",
       "braycurtis        NaN   NaN\n",
       "intersection      NaN   NaN\n",
       "bhattacharyya     NaN   NaN\n",
       "chi2              NaN   NaN\n",
       "emd               NaN   NaN"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "resultDF=pd.DataFrame(index=methodName,columns=[\"Correct\", \"Error\"])\n",
    "resultDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following outer loop iterates over all distance-metrics defined in list *methods*. For each metric first the distance for all pairs of images is calculated and stored in the 2-dimensional array *dist*. The entry in row i, column j of this matrix stores the distance between the histograms of the i.th and the j.th image. Applying numpyâ€™s *argsort()* method on each row of the 2 dimensional array *dist* another 2-dimensional array *s* is calculated. The i.th row of this matrix contains the indicees of all images, sorted w.r.t. increasing distance to image *i*. I.e. in the first column of the i.th row of s is the index *i* and in the second column is the index of the image, which matches best with image *i*. In order to determine if the recognition is correct, one must just check if the index in the second column of *s* belongs to the same object as the index of the corresponding row. This can be done by a simple integer division, since the images are indexed such that indicees 0 to 4 belong to the same object, indicees 5 to 9 belong to the next object, and so on. The results are saved in the pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.arange(0,features.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Correct</th>\n",
       "      <th>Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>euclidean</th>\n",
       "      <td>181</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correlation</th>\n",
       "      <td>175</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>canberra</th>\n",
       "      <td>257</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>braycurtis</th>\n",
       "      <td>222</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intersection</th>\n",
       "      <td>222</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bhattacharyya</th>\n",
       "      <td>243</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chi2</th>\n",
       "      <td>239</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emd</th>\n",
       "      <td>159</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Correct Error\n",
       "euclidean         181    84\n",
       "correlation       175    90\n",
       "canberra          257     8\n",
       "braycurtis        222    43\n",
       "intersection      222    43\n",
       "bhattacharyya     243    22\n",
       "chi2              239    26\n",
       "emd               159   106"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for j,m in enumerate(methods):\n",
    "    dist=np.zeros((N_IM,N_IM))\n",
    "    for h1 in range(N_IM):\n",
    "        for h2 in range(h1,N_IM):\n",
    "           if j == len(methods)-1:\n",
    "            dist[h1,h2]=wasserstein_distance(values,values,features[h1,:],features[h2,:])\n",
    "           else: \n",
    "            dist[h1,h2]=m(features[h1,:],features[h2,:])\n",
    "           dist[h2,h1]=dist[h1,h2]\n",
    "    s=np.argsort(dist,axis=1)\n",
    "    correct=0\n",
    "    error=0\n",
    "    for i in range(N_IM):\n",
    "        for a in s[i,1:2]:\n",
    "            if int(a/5) != int(i/5):\n",
    "                error+=1\n",
    "            else:\n",
    "                correct+=1\n",
    "    resultDF.loc[methodName[j],\"Correct\"]=correct\n",
    "    resultDF.loc[methodName[j],\"Error\"]=error\n",
    "resultDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use pretrained CNNs for Retrieval\n",
    "Convolutional Neural Networks (CNN) consist of a feature-extractor part and a classifier. The last part of the CNN (blue part in the image below) is the classifier, consisting one or more dense layers. All other layers (orange part in the image below) constitute the feature extractor part, usually realised by convolutional and pooling layers.\n",
    "<img src=\"https://maucher.home.hdm-stuttgart.de/Pics/dnnExtractorClassifier.png\" style=\"width:600px\" align=\"middle\">\n",
    "In a well trained CNN, the feature-extractor part provides a good representation of the input image to the classifier. \n",
    "\n",
    "In the context of image retrieval one can apply the pretrained feature-extractor part to calculate a good representation of all images - hopefully this representation is better than e.g. a global color-histogram. Instead of comparing color histograms of images in order to find the best matching image, one can compare these dnn-generated features for image retrieval.\n",
    "\n",
    "In the remainder of this notebook we apply different pretrained deep neural networks to calculate good representations. Based on these representations best matching image-pairs are determined in the same way as above. All CNNs have been pretrained on the imagenet-dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import vgg16, inception_v3, resnet50, mobilenet\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "#from tensorflow.keras.applications.imagenet_utils import decode_predictions\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load pretrained CNNs without the classifier part, i.e. only the feature-extractor part:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
     ]
    }
   ],
   "source": [
    "vgg_model = vgg16.VGG16(weights='imagenet',include_top=False)\n",
    "inception_model = inception_v3.InceptionV3(weights='imagenet',include_top=False)\n",
    "resnet_model = resnet50.ResNet50(weights='imagenet',include_top=False)\n",
    "mobilenet_model = mobilenet.MobileNet(weights='imagenet',include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMGSIZE_X=224\n",
    "IMGSIZE_Y=224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(265, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "image_batch=np.zeros((len(imlist),IMGSIZE_X,IMGSIZE_Y,3))\n",
    "for i,filename in enumerate(imlist):\n",
    "    #im = np.array(Image.open(f))\n",
    "    original = load_img(filename, target_size=(IMGSIZE_X,IMGSIZE_Y))\n",
    "    numpy_image = img_to_array(original)\n",
    "    image_batch[i,:,:,:] = np.expand_dims(numpy_image, axis=0)\n",
    "print(image_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets=[vgg16, inception_v3, resnet50, mobilenet]\n",
    "models=[vgg_model, inception_model, resnet_model, mobilenet_model]\n",
    "modelNames=[\"vgg\",\"inception\",\"resnet\",\"mobilenet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Correct</th>\n",
       "      <th>Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vgg</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inception</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mobilenet</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Correct Error\n",
       "vgg           NaN   NaN\n",
       "inception     NaN   NaN\n",
       "resnet        NaN   NaN\n",
       "mobilenet     NaN   NaN"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "resultDF2=pd.DataFrame(index=modelNames,columns=[\"Correct\", \"Error\"])\n",
    "resultDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric=correlation #euclidean\n",
    "for net,model,name in zip(nets,models,modelNames):\n",
    "    processed_image = net.preprocess_input(image_batch.copy())\n",
    "    predictions = model.predict(processed_image)\n",
    "    features=predictions.reshape(265,-1)\n",
    "    #modelDict[name][\"features\"]=features\n",
    "    \n",
    "    dist=np.zeros((N_IM,N_IM))\n",
    "    for h1 in range(N_IM):\n",
    "        for h2 in range(h1,N_IM):\n",
    "           dist[h1,h2]=metric(features[h1,:],features[h2,:])\n",
    "           dist[h2,h1]=dist[h1,h2]\n",
    "    s=np.argsort(dist,axis=1)\n",
    "    correct=0\n",
    "    error=0\n",
    "    for i in range(N_IM):\n",
    "        for a in s[i,1:2]:\n",
    "            if int(a/5) != int(i/5):\n",
    "                error+=1\n",
    "            else:\n",
    "                correct+=1\n",
    "    resultDF2.loc[name,\"Correct\"]=correct\n",
    "    resultDF2.loc[name,\"Error\"]=error\n",
    "    #print(\"Name: %s: \\t Correct: %3d \\t Error: %3d\"%(name,correct,error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Correct</th>\n",
       "      <th>Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>vgg</th>\n",
       "      <td>248</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inception</th>\n",
       "      <td>248</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resnet</th>\n",
       "      <td>260</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mobilenet</th>\n",
       "      <td>253</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Correct Error\n",
       "vgg           248    17\n",
       "inception     248    17\n",
       "resnet        260     5\n",
       "mobilenet     253    12"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
