
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Local Image Features &#8212; Object Recognition Lecture</title>
    
  <link rel="stylesheet" href="../_static/css/index.73d71520a4ca3b99cfee5594769eaaae.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.3da636dd464baa7582d2.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Example: Harris-Förstner Corner Detection" href="harrisCornerDetection.html" />
    <link rel="prev" title="Example: Naive Bayes Object Recognition" href="probRecognition.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/hdmlogomed.jpg" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Object Recognition Lecture</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Intro and Overview Object Recognition Lecture
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Image Processing
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../preprocessing/01accessImage.html">
   Basic Image Access Operations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../preprocessing/02filtering.html">
   Basic Filter Operations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../preprocessing/04gaussianDerivatives.html">
   Gaussian Filter and Derivatives of Gaussian
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../preprocessing/03LowPassFilter.html">
   Rectangular- and Gaussian Low Pass Filtering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../preprocessing/06GaussianNoiseReduction.html">
   Noise Suppression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../preprocessing/05GaussianLowPassFilter.html">
   Gaussian and Difference of Gaussian Pyramid
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Features
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="globalDescriptors.html">
   Global Image Features
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="similarityMetrics.html">
   Similarity Measures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ImageRetrieval.html">
   Histogram-based Image Retrieval
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ImageRetrieval.html#use-pretrained-cnns-for-retrieval">
   Use pretrained CNNs for Retrieval
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="multiReceptiveFields.html">
   Multidimensional Receptive Field Histograms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="naiveBayesHistogram.html">
   Histogram-based Naive Bayes Recognition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="probRecognition.html">
   Example: Naive Bayes Object Recognition
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Local Image Features
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="harrisCornerDetection.html">
   Example: Harris-Förstner Corner Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="siftDescriptorCV2.html">
   Example: Create SIFT Descriptors with openCV
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="HoGfeatures.html">
   Histogram of Oriented Gradients: Step-by-Step
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="HOGpedestrianDetection.html">
   HOG-based Pedestrian Detection
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Object Recognition
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../recognition/objectrecognition.html">
   Object Recognition
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Deep Learning
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../deeplearning/ConvolutionNeuralNetworks.html">
   Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../deeplearning/convolutionDemos.html">
   Animations of Convolution and Deconvolution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../deeplearning/cnns.html">
   Convolutional Neural Networks for Object Recognition
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Face Detection and Recognition
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../face/faceDetection.html">
   Face Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../face/faceRecognition.html">
   Face Recognition using FaceNet
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Pose Estimation
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../poseEstimation/Pose_Estimation.html">
   Multi-Person 2D Pose Estimation using Part Affinity Fields
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="../referenceSection.html">
   References
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/features/localFeatures.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#general-requirements-and-categorization">
   General Requirements and Categorization
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#harris-forstner-corner-detection">
   Harris-Förstner Corner Detection
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#scale-invariant-feature-transform-sift">
   Scale Invariant Feature Transform (SIFT)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#scale-space-extrema-detection">
     Scale space extrema detection
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#keypoint-localization-and-filtering">
     Keypoint localization and filtering
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#exact-keypoint-localization">
       Exact Keypoint Localization
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#reject-low-contrast-candidates">
       Reject Low Contrast Candidates
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#reject-candidates-on-edges">
       Reject Candidates on Edges
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#orientation-assignment">
     Orientation Assignment
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#create-orientation-histogram">
       Create Orientation Histogram
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#assign-orientations-to-keypoints">
       Assign Orientations to Keypoints
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#keypoint-descriptor">
     Keypoint descriptor
    </a>
   </li>
  </ul>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="local-image-features">
<h1>Local Image Features<a class="headerlink" href="#local-image-features" title="Permalink to this headline">¶</a></h1>
<p>In contrast to global features, local features describe not the entire image, but only a local region within the image. As depicted in the figure below, for each image usually a large set of local descriptors is obtained. Actually in the image below, not the <strong>local descriptors</strong> itself, but the <strong>keypoints</strong> are shown by blue markers. Usually one first finds keypoints, then the area around each keypoint is described by a local descriptor, which is usually a numeric vector.</p>
<img alt="https://maucher.home.hdm-stuttgart.de/Pics/a4sift.png" class="align-center" src="https://maucher.home.hdm-stuttgart.de/Pics/a4sift.png" />
<p>In most applications, local features provide much more robustness than global features. Depending on the specific type of local feature, they can be robust w.r.t. rotation, translation, viewpoint, scale, ligthing partial deformation and partial occlusion.</p>
<p>In <a class="bibtex reference internal" href="../referenceSection.html#graumanleibe" id="id1">[GL11]</a> local features are defined as follows:</p>
<div class="admonition-local-features admonition">
<p class="admonition-title">Local Features</p>
<p>The purpose of local invariant features is to provide a representation that allows to efficiently match local structures between images. That is, we want to obtain a sparse set of local measurements that capture the essence of the underlying input images and that encode their interesting structure.</p>
</div>
<img alt="https://maucher.home.hdm-stuttgart.de/Pics/vishouse2.jpg" class="align-center" src="https://maucher.home.hdm-stuttgart.de/Pics/vishouse2.jpg" />
<p>Image Source: <a class="reference external" href="http://cs.brown.edu/courses/cs143/results/proj2/sphene">http://cs.brown.edu/courses/cs143/results/proj2/sphene/</a></p>
<div class="section" id="general-requirements-and-categorization">
<h2>General Requirements and Categorization<a class="headerlink" href="#general-requirements-and-categorization" title="Permalink to this headline">¶</a></h2>
<p>According to <a class="bibtex reference internal" href="../referenceSection.html#tuyte07" id="id2">[TM07]</a> local features in general shall have the following characteristics:</p>
<ul class="simple">
<li><p><strong>Repeatability</strong>: Given two images of the same object, taken under different viewing conditions, a high percentage of the features detected on the object part visible in both images should be found in both images.</p></li>
</ul>
<img alt="https://maucher.home.hdm-stuttgart.de/Pics/differentEdgesRed.png" class="align-center" src="https://maucher.home.hdm-stuttgart.de/Pics/differentEdgesRed.png" />
<ul class="simple">
<li><p><strong>Distinctiveness/informativeness</strong>: The intensity patterns underlying the detected features should show a lot of variation, such that features can be distinguished and matched. By looking through a small window it must be easy to localize the point. Shifting the window in any direction should give a large change in pixel intensities, gradients, …</p></li>
</ul>
<img alt="https://maucher.home.hdm-stuttgart.de/Pics/properKeypoints.jpg" class="align-center" src="https://maucher.home.hdm-stuttgart.de/Pics/properKeypoints.jpg" />
<ul class="simple">
<li><p><strong>Locality</strong>: The features should be local, so as to reduce the probability of occlusion and to allow simple model approximations of the geometric and photometric deformations
between two images taken under different viewing conditions.</p></li>
<li><p><strong>Quantity</strong>: The number of detected features should be sufficiently large, such that a reasonable number of features are detected even on small objects. However, the optimal number
of features depends on the application. Ideally, the number
of detected features should be adaptable over a large range
by a simple and intuitive threshold. The density of features
should reflect the information content of the image to provide
a compact image representation.</p></li>
<li><p><strong>Accuracy</strong>: The detected features should be accurately localized, both in image location, as with respect to scale and possibly shape.</p></li>
<li><p><strong>Efficiency</strong>: Preferably, the detection of features in a new image should allow for time-critical applications.</p></li>
</ul>
<p>Applications, which integrate local features can be categorized in an abstract level as follows (<a class="bibtex reference internal" href="../referenceSection.html#tuyte07" id="id3">[TM07]</a>):</p>
<ol class="simple">
<li><p>Interest in a <strong>specific type of local feature</strong> that has a clear semantic interpretation in the context of a given application. E.g. in aerial images edges indicate roads.</p></li>
<li><p>Interest in local features since they <strong>provide a limited set of well localized and individually identifiable anchor points</strong> (e.g. for tracking applications or image stichting).</p></li>
<li><p>Use set of local features as a <strong>robust image representation</strong>. Allows for object- or scene recognition without image segmentation.</p></li>
</ol>
<p>In the second and third category the semantics of the local feature are not relevant.</p>
<p>An example for the second category is <strong>image stitching:</strong></p>
<p>First keypoints are detected in both images. Then pairs of corresponding keypoints must be found. The locations of the matching pairs are then used to align the images.</p>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/keypointsInMountainPics.PNG" style="width:600px" align="center">
</figure>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/alignedMountainPics.PNG" style="width:600px" align="center">
    <figcaption>
        Example Autostichting
    </figcaption>
</figure>
<p>An example for the third category is <strong>object identification:</strong> First local features from both images are extracted independently. Then candidate matching pairs of descriptors must be determined. From the location of the matching pairs the most probable geometric configuration can be determined and verified.</p>
<img alt="https://maucher.home.hdm-stuttgart.de/Pics/siftRecognLorry.PNG" class="align-center" src="https://maucher.home.hdm-stuttgart.de/Pics/siftRecognLorry.PNG" />
</div>
<div class="section" id="harris-forstner-corner-detection">
<h2>Harris-Förstner Corner Detection<a class="headerlink" href="#harris-forstner-corner-detection" title="Permalink to this headline">¶</a></h2>
<p>As stated above an important <strong>criteria for good keypoints</strong> is:</p>
<blockquote class="epigraph">
<div><p>Shifting the small window in any direction should give a large change in gradients.</p>
</div></blockquote>
<p>According to <a class="bibtex reference internal" href="../referenceSection.html#harris88" id="id4">[HS88]</a> the <strong>change of gradients</strong> in the neighborhood of a pixel can be determined as follows:</p>
<ol>
<li><p>Calculate the gradient at position <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-gradim">
<span class="eqno">(66)<a class="headerlink" href="#equation-gradim" title="Permalink to this equation">¶</a></span>\[\begin{split}
    \nabla I(\mathbf{x}) = \left(
    \begin{array}{c}
    \frac{\partial I(\mathbf{x})}{\partial x} \\
    \frac{\partial I(\mathbf{x})}{\partial y}
    \end{array}
    \right)
    =
    \left(
    \begin{array}{c}
    I_x(\mathbf{x}) \\
    I_y(\mathbf{x})
    \end{array}
    \right)
    \end{split}\]</div>
<p>The gradients are calculated by applying the 1st order derivative of a Gaussian (see chapter Image Processing).</p>
</li>
<li><p>At each position <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> calculate the matrix</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\begin{split}
    M_I(\mathbf{x}) = \nabla I(\mathbf{x}) \nabla I^T(\mathbf{x}) = \left(
    \begin{array}{c}
    I_x(\mathbf{x}) \\
    I_y(\mathbf{x})
    \end{array}
    \right) \left( I_x(\mathbf{x}) \quad I_y(\mathbf{x}) \right)\end{split}\\\begin{split}    = \left(
    \begin{array}{cc}
    I_x^2(\mathbf{x}) &amp; I_x I_y(\mathbf{x}) \\
    I_x I_y(\mathbf{x}) &amp; I_y^2(\mathbf{x}) 
    \end{array}
    \right)
    \end{split}\end{aligned}\end{align} \]</div>
</li>
<li><p>Average matrices <span class="math notranslate nohighlight">\(M_I(\mathbf{x})\)</span> over a region by convolution with a Gaussian <span class="math notranslate nohighlight">\(G_{\sigma}\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-aver">
<span class="eqno">(67)<a class="headerlink" href="#equation-aver" title="Permalink to this equation">¶</a></span>\[
    C(\mathbf{x},\sigma)=G_{\sigma}(\mathbf{x}) * M_I(\mathbf{x})
    \]</div>
</li>
<li><p>Depending on the local image properties in the region defined by the width of <span class="math notranslate nohighlight">\(G_{\sigma}\)</span> the <strong>Eigenvalues <span class="math notranslate nohighlight">\(\lambda_1\)</span> and <span class="math notranslate nohighlight">\(\lambda_2\)</span></strong> of matrix <span class="math notranslate nohighlight">\(C(\mathbf{x},\sigma)\)</span> vary as follows:</p>
<ul class="simple">
<li><p>If both Eigenvalues are large, then there is a corner around <span class="math notranslate nohighlight">\(\mathbf{x}\)</span></p></li>
<li><p>If one Eigenvalue is large, and the other is <span class="math notranslate nohighlight">\(\approx 0\)</span>, then there is an edge around <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p></li>
<li><p>If both Eigenvalues are <span class="math notranslate nohighlight">\(\approx 0\)</span>, then there is no variation in the region around <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>.</p></li>
</ul>
</li>
</ol>
<p>Note that <span class="math notranslate nohighlight">\(C(\mathbf{x},\sigma)\)</span> and it’s Eigenvalues must be calculated at each position <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and the calculation of Eigenvalues is costly. However, there is a <strong>trick to perform the differentiation, mentioned in item 4 above, without calculating the Eigenvalues.</strong> For this let <span class="math notranslate nohighlight">\(\alpha\)</span> be the Eigenvalue with the larger and <span class="math notranslate nohighlight">\(\beta\)</span> be the Eigenvalue with the smaller magnitude. Morevoer,</p>
<div class="math notranslate nohighlight">
\[
r=\frac{\alpha}{\beta}
\]</div>
<p>is the ratio of these Eigenvalues.</p>
<ul>
<li><p>Calculate <a class="reference external" href="https://en.wikipedia.org/wiki/Trace_(linear_algebra)">trace</a> and <a class="reference external" href="https://en.wikipedia.org/wiki/Determinant">determinant</a> of <span class="math notranslate nohighlight">\(C(\mathbf{x},\sigma)\)</span></p>
<div class="math notranslate nohighlight">
\[
    Tr(\mathbf{C(\mathbf{x},\sigma)}) =  \alpha + \beta 
    \]</div>
<div class="math notranslate nohighlight">
\[
    Det(\mathbf{C(\mathbf{x},\sigma)}) =  \alpha  \beta
    \]</div>
</li>
<li><p>Then</p>
<div class="math notranslate nohighlight" id="equation-tracedet">
<span class="eqno">(68)<a class="headerlink" href="#equation-tracedet" title="Permalink to this equation">¶</a></span>\[
    \frac{Tr(\mathbf{C(\mathbf{x},\sigma)})^2}{Det(\mathbf{C(\mathbf{x},\sigma)})} = \frac{(\alpha + \beta)^2}{\alpha \beta} = \frac{(r \beta + \beta)^2}{r\beta^2}=\frac{(r+1)^2}{r}
    \]</div>
<p>depends only on the ratio <span class="math notranslate nohighlight">\(r\)</span> of eigenvalues.</p>
</li>
<li><p>Corners are at the points where this value is small.</p></li>
<li><p>In order to check if the value in equation <a class="reference internal" href="#equation-tracedet">(68)</a> is small one can equivalently check if</p></li>
</ul>
<div class="math notranslate nohighlight" id="equation-cornertest">
<span class="eqno">(69)<a class="headerlink" href="#equation-cornertest" title="Permalink to this equation">¶</a></span>\[
Det(\mathbf{C(\mathbf{x},\sigma)}) - \kappa Tr(\mathbf{C(\mathbf{x},\sigma)})^2 &gt; T.
\]</div>
<p>Concerning the parameters in in the described process:</p>
<ul class="simple">
<li><p><strong><span class="math notranslate nohighlight">\(\kappa\)</span></strong> is in the range <span class="math notranslate nohighlight">\([0.04,\ldots 0.15]\)</span>. Smaller <span class="math notranslate nohighlight">\(\kappa\)</span> yields more detected corners.</p></li>
<li><p>If the standard deviation of the Gaussian filter, used to calculate the derivatives in equation <a class="reference internal" href="#equation-gradim">(66)</a> is <span class="math notranslate nohighlight">\(\sigma_d\)</span>, then the standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> of the Gaussian, used in equation <a class="reference internal" href="#equation-aver">(67)</a>, should be <strong><span class="math notranslate nohighlight">\(\sigma \approx 2\sigma_d\)</span></strong>. Typical value: <span class="math notranslate nohighlight">\(\sigma=1.0\)</span> i.e. <span class="math notranslate nohighlight">\(\sigma_d \approx 0.5\)</span>.</p></li>
</ul>
<p>In implementations, e.g. <a class="reference external" href="https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.corner_harris">Scikits Image</a>, usually one function calculates for all pixels the value, given in the left hand side of unequation <a class="reference internal" href="#equation-cornertest">(69)</a> and a second function applies the <strong>threshold <span class="math notranslate nohighlight">\(T\)</span></strong> (default value 0) and a <strong>minimum distance <span class="math notranslate nohighlight">\(D_{min}\)</span></strong> to the result of the first function. Detected corners are then separated by at least <span class="math notranslate nohighlight">\(D_{min}\)</span> pixels. The influence of varying parameters <span class="math notranslate nohighlight">\(\kappa\)</span> and <span class="math notranslate nohighlight">\(D_{min}\)</span> is demonstrated in the picture below):</p>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/harrisCornerAll.png" style="width:600px" align="center">
</figure>
<p>In section <a class="reference internal" href="harrisCornerDetection.html"><span class="doc std std-doc">harrisCornerDetection</span></a> an implementation of Harris-Corner detection is demonstrated.</p>
<p>Even though Harris-Förstner corner detection is translation- and rotation-invariant, one major drawback is that it is <strong>not scale-invariant</strong>. As depicted in the picture below, the approach can find corners if the object is given in a small scale. However, if the object is represented in a higher scale there may be now detected corners in the same object-area.</p>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/harrisScale.PNG" style="width:400px" align="center">
</figure>
<p>An approach for scale-invariant features is described in the following subsection.</p>
</div>
<div class="section" id="scale-invariant-feature-transform-sift">
<h2>Scale Invariant Feature Transform (SIFT)<a class="headerlink" href="#scale-invariant-feature-transform-sift" title="Permalink to this headline">¶</a></h2>
<p>Scale Invariant Feature Transform (SIFT) is an algorithm to detect and describe local features in images. It has been introduced in <a class="bibtex reference internal" href="../referenceSection.html#lowe04" id="id5">[Low04]</a>. SIFT features are invariant w.r.t. translation, rotation, scale and quite robust w.r.t. illumination, 3D viewpoint, additional noise and partial occlusion. The SIFT process consists of the following steps:</p>
<div class="sphinx-bs container pb-4 docutils">
<div class="row docutils">
<div class="d-flex col-lg-6 col-md-6 col-sm-6 col-xs-12 p-2 docutils">
<div class="card w-100 shadow docutils">
<div class="card-header docutils">
<p class="card-text">Phase 1: Scale space extrema detection</p>
</div>
<div class="card-body docutils">
<p class="card-text">Search potential keypoints by determining extremas in scale space.</p>
</div>
</div>
</div>
<div class="d-flex col-lg-6 col-md-6 col-sm-6 col-xs-12 p-2 docutils">
<div class="card w-100 shadow docutils">
<div class="card-header docutils">
<p class="card-text">Phase 2: Keypoint localization and filtering</p>
</div>
<div class="card-body docutils">
<p class="card-text">Keypoint localization and filtering</p>
</div>
</div>
</div>
<div class="d-flex col-lg-6 col-md-6 col-sm-6 col-xs-12 p-2 docutils">
<div class="card w-100 shadow docutils">
<div class="card-header docutils">
<p class="card-text">Phase 3: Orientation Assignment</p>
</div>
<div class="card-body docutils">
<p class="card-text">One or more orientations are assigned to each keypoint location.</p>
</div>
</div>
</div>
<div class="d-flex col-lg-6 col-md-6 col-sm-6 col-xs-12 p-2 docutils">
<div class="card w-100 shadow docutils">
<div class="card-header docutils">
<p class="card-text">Phase 4: Keypoint descriptor</p>
</div>
<div class="card-body docutils">
<p class="card-text">Local image gradients are measured at the selected scale in the region around each keypoint</p>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="scale-space-extrema-detection">
<h3>Scale space extrema detection<a class="headerlink" href="#scale-space-extrema-detection" title="Permalink to this headline">¶</a></h3>
<ul>
<li><p><strong>Scale Space</strong> of input image <span class="math notranslate nohighlight">\(I(x,y)\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    L(x,y,\sigma)=G(x,y,\sigma) * I(x,y)
    \]</div>
</li>
<li><p><strong>Variable Scale Gaussian</strong>:</p>
<div class="math notranslate nohighlight">
\[
    G(x,y,\sigma)= \frac{1}{2\pi\sigma^2}e^{-\dfrac{x^2+y^2}{2\sigma^2}}
    \]</div>
</li>
<li><p><strong>Difference of Gaussian (DoG)</strong>:</p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-d51a2cd2-9795-4854-87ce-b75bd98b65ef">
<span class="eqno">(70)<a class="headerlink" href="#equation-d51a2cd2-9795-4854-87ce-b75bd98b65ef" title="Permalink to this equation">¶</a></span>\[\begin{eqnarray}
D(x,y,\sigma) &amp; = &amp; \left(G(x,y,k\sigma)-G(x,y,\sigma)\right) * I(x,y)  \nonumber \\
              &amp; = &amp; L(x,y,k\sigma)-L(x,y,\sigma).
\end{eqnarray}\]</div>
<ul class="simple">
<li><p><strong>DoG</strong> has already been introduced in <a class="reference internal" href="../preprocessing/05GaussianLowPassFilter.html"><span class="doc std std-doc">Gaussian and Difference of Gaussian Pyramid</span></a>. It is</p>
<ul>
<li><p>efficient to compute - the smoothed images L must be computed in any case for the scale space feature description</p></li>
<li><p>a close approximation to the <strong>scale-normalized Laplacian of Gaussian</strong>, whose extremas produce the most stable image features (<a class="bibtex reference internal" href="../referenceSection.html#lowe04" id="id6">[Low04]</a>)</p></li>
</ul>
</li>
</ul>
<p>Scale space and the DOGs are visualized in the image below (Source: <a class="bibtex reference internal" href="../referenceSection.html#lowe04" id="id7">[Low04]</a>). An <em>octave</em> means doubling of <span class="math notranslate nohighlight">\(\sigma\)</span>. The scale space resolution is <span class="math notranslate nohighlight">\(k=2^{1/s}\)</span>, where <span class="math notranslate nohighlight">\(s\)</span> is the number of scales per octave. After each octave the image is subsampled by a factor of <span class="math notranslate nohighlight">\(2\)</span>. For each octave <span class="math notranslate nohighlight">\(s+3\)</span> blurred images are required to determine scale space extrema (see next slide). A typical resolution is <span class="math notranslate nohighlight">\(s=3\)</span>.</p>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/loweSIFTdoG.PNG" style="width:500px" align="center">
</figure>
<p>The scale space of a concrete image is depicted below:</p>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/scaleSpace.PNG" style="width:500px" align="center">
</figure>
<p>As shown in the image below, in the DoG each pixel is compared with its 26 neighbour pixels at the current and adjacent scales. The Pixel is selected as a <strong>keypoint candidate</strong>, if it is larger than all of the neighbours (maxima) or smaller than all of them (minima).</p>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/loweSiftextrema.PNG" style="width:700px" align="center">
</figure>
</div>
<div class="section" id="keypoint-localization-and-filtering">
<h3>Keypoint localization and filtering<a class="headerlink" href="#keypoint-localization-and-filtering" title="Permalink to this headline">¶</a></h3>
<p>The set of keypoint candidates is refined as follows:</p>
<ul class="simple">
<li><p>Determine <strong>exact position of local extremas</strong> on sub-pixel resolution</p></li>
<li><p>Select within the set of candidate keypoints only the <strong>stable and distinctive</strong> ones. Candidate keypoints with low contrast are not stable, because they are sensitive to noise. Moreover, candidate keypoints on edges are not distinctive.</p></li>
</ul>
<div class="section" id="exact-keypoint-localization">
<h4>Exact Keypoint Localization<a class="headerlink" href="#exact-keypoint-localization" title="Permalink to this headline">¶</a></h4>
<p>In the previous step candidate keypoints have been localized on pixel level. Note that particularly for candidates on a high level of the DoG (where images are at a low-resolution) a localization on pixel-level is quite inaccurate. Therefore, the keypoint-localization must be improved to sub-pixel level. In general there exist two possibilities to estimate sub-pixel information. The first is <strong>interpolation</strong>, the second is by <strong>Taylor-series expansion</strong>.</p>
<p>The idea of interpolation is depicted in the image below:</p>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/interpolationSIFT.png" style="width:500px" align="center">
</figure>
<p>However, in SIFT Taylor-series expansion is applied for refining keypoint localization.</p>
<p><strong>Taylor expansion</strong> of <span class="math notranslate nohighlight">\(D(x,y,\sigma)\)</span> up to quadratic forms around the sample point (keypoint candidate) is defined as follows:</p>
<div class="math notranslate nohighlight" id="equation-taylor">
<span class="eqno">(71)<a class="headerlink" href="#equation-taylor" title="Permalink to this equation">¶</a></span>\[
D(\mathbf{x})=D+\frac{\partial D^T}{\partial \mathbf{x}}\mathbf{x}+\frac{1}{2} \mathbf{x}^T \frac{\partial^2 D}{\partial \mathbf{x}^2}\mathbf{x}.
\]</div>
<p>Here <span class="math notranslate nohighlight">\(\mathbf{x}=(x,y,\sigma)^T\)</span> is the offset from the candidate keypoint and the first and second derivation are:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial D}{\partial \mathbf{x}} = \left(\begin{array}{c} \frac{\partial D}{\partial x} \\ 
\frac{\partial D}{\partial y} \\
\frac{\partial D}{\partial \sigma}
\end{array} \right) = \left(\begin{array}{c}
D_x \\
D_y \\
D_{\sigma}
\end{array} \right)
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial^2 D}{\partial \mathbf{x}^2}=\left(\begin{array}{ccc}
D_{xx} &amp; D_{xy} &amp; D_{x \sigma}\\
D_{yx} &amp; D_{yy} &amp; D_{y \sigma}\\
D_{\sigma x} &amp; D_{\sigma y} &amp; D_{\sigma \sigma}\\
\end{array} \right)
\end{split}\]</div>
<p>The derivatives of <span class="math notranslate nohighlight">\(D\)</span> are approximated by calculating differences in the <span class="math notranslate nohighlight">\((3x3)\)</span>-neighbourhood.
The <strong>true location <span class="math notranslate nohighlight">\(\hat{\mathbf{x}}\)</span></strong> of the keypoint is determined by taking the derivative of <a class="reference internal" href="#equation-taylor">(71)</a> and setting it to zero:</p>
<div class="math notranslate nohighlight">
\[
\hat{\mathbf{x}}= - \frac{\partial^2 D}{\partial \mathbf{x}^2}^{-1} \frac{\partial D}{\partial \mathbf{x}}.
\]</div>
<p>If the offset <span class="math notranslate nohighlight">\(\hat{\mathbf{x}}\)</span> is <span class="math notranslate nohighlight">\(&gt;0.5\)</span> in any dimension, then the extremum lies closer to different sample point and new interpolation starts centered at this new point.</p>
</div>
<div class="section" id="reject-low-contrast-candidates">
<h4>Reject Low Contrast Candidates<a class="headerlink" href="#reject-low-contrast-candidates" title="Permalink to this headline">¶</a></h4>
<p>Pixels of low contrast can be determined by low DoG values. If the image values are normalized to the range <span class="math notranslate nohighlight">\(\left[0,1\right]\)</span>, candidates with</p>
<div class="math notranslate nohighlight">
\[
|D(\hat{\mathbf{x}})|&lt;0.03
\]</div>
<p>are rejected as low contrast points.</p>
</div>
<div class="section" id="reject-candidates-on-edges">
<h4>Reject Candidates on Edges<a class="headerlink" href="#reject-candidates-on-edges" title="Permalink to this headline">¶</a></h4>
<p>DoG values can not uniquely locate points on edges (since all points on the edge have similar DoG values), but in corners.
Since edges constitute extremas in the DoG and are therefore selected as candidates in the first step, they must be filtered out now.
The idea of the edge-rejection approach in SIFT is based on the fact, that at edges the DoG function will have a <strong>large prinicpal curvature across the edge</strong>, but a relatively small curvature in the orthogonal direction (along the edge). Keypoint candidates, which fulfill this criteria are rejected.</p>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/principalCurvatures.png" style="width:500px" align="center">
</figure>
<p>The curvature-test is implemented as follows:</p>
<p>The two <strong>eigenvalues</strong> of the 2D <strong>Hessian</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathbf{H}=\left( \begin{array}{cc}
D_{xx} &amp; D_{xy} \\
D_{yx} &amp; D_{yy} 
\end{array}
\right)
\end{split}\]</div>
<p>are proportional to the principal curvatures.</p>
<p>Denote:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span>: eigenvalue with larger magnitude</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta\)</span>: eigenvalue with smaller magnitude</p></li>
</ul>
<p>A large ratio</p>
<div class="math notranslate nohighlight">
\[
r=\frac{\alpha}{\beta}
\]</div>
<p>indicates a large curvature in one direction and a small in the perpendicular direction <strong><span class="math notranslate nohighlight">\(\Rightarrow\)</span> Edge</strong>.</p>
<ul class="simple">
<li><p>Calculate trace and determinant of <span class="math notranslate nohighlight">\(\mathbf{H}\)</span></p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-4c4d51ce-e2b7-4efd-900d-a8187c3733a9">
<span class="eqno">(72)<a class="headerlink" href="#equation-4c4d51ce-e2b7-4efd-900d-a8187c3733a9" title="Permalink to this equation">¶</a></span>\[\begin{eqnarray}
Tr(\mathbf{H}) &amp; = D_{xx}+D_{yy} = \alpha + \beta \\
Det(\mathbf{H}) &amp; = D_{xx} D_{yy} -(D_{xy})^2 = \alpha  \beta
\end{eqnarray}\]</div>
<p>then</p>
<div class="amsmath math notranslate nohighlight" id="equation-1293a59b-e0b1-4a07-b2d1-84e95a4d98ef">
<span class="eqno">(73)<a class="headerlink" href="#equation-1293a59b-e0b1-4a07-b2d1-84e95a4d98ef" title="Permalink to this equation">¶</a></span>\[\begin{equation}
\frac{Tr(\mathbf{H})^2}{Det(\mathbf{H})} = \frac{(\alpha + \beta)^2}{\alpha \beta} = \frac{(r \beta + \beta)^2}{r\beta^2}=\frac{(r+1)^2}{r}
\end{equation}\]</div>
<p>depends only on the ratio <span class="math notranslate nohighlight">\(r\)</span> of eigenvalues.</p>
<p>In <a class="bibtex reference internal" href="../referenceSection.html#lowe04" id="id8">[Low04]</a> candidates with <span class="math notranslate nohighlight">\(r&gt;10\)</span> are rejected as edges.</p>
<p>The following picture displays the contrast-dependent and the curvature-dependent candidate rejection.</p>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/loweSIFThouseExample.PNG" style="width:650px" align="center">
</figure>
</div>
</div>
<div class="section" id="orientation-assignment">
<h3>Orientation Assignment<a class="headerlink" href="#orientation-assignment" title="Permalink to this headline">¶</a></h3>
<p>This step is required in order to provide an image-descriptor, which is <em>invariant to image-rotation</em>. The idea is to calculate for each keypoint, remained from the previous steps, an <strong>orientation</strong> from it’s local image properties.</p>
<div class="section" id="create-orientation-histogram">
<h4>Create Orientation Histogram<a class="headerlink" href="#create-orientation-histogram" title="Permalink to this headline">¶</a></h4>
<ol>
<li><p>From the scale of the keypoint, select the Gaussian smoothed image, <span class="math notranslate nohighlight">\(L\)</span>, with the closest scale.</p></li>
<li><p>For each image sample, <span class="math notranslate nohighlight">\(L(x,y)\)</span> at the selected scale compute <strong>gradient magnitude:</strong></p>
<div class="math notranslate nohighlight">
\[
    m(x,y)=\sqrt{(L(x+1,y)-L(x-1,y))^2+(L(x,y+1)-L(x,y-1))^2}
    \]</div>
<p>and <strong>gradient orientation</strong></p>
<div class="math notranslate nohighlight">
\[
    \theta(x,y)=\tanh\frac{L(x,y+1)-L(x,y-1)}{L(x+1,y)-L(x-1,y)}
    \]</div>
</li>
<li><p>Calculate a 36 bin gradient histogram (<span class="math notranslate nohighlight">\( 1 bin \triangleq 10°\)</span>) from the gradients of the samples in the local neighbourhood of the keypoint. The contribution of each gradient to the histogram is the product of</p>
<ul class="simple">
<li><p>its magnitude <span class="math notranslate nohighlight">\(m(x,y)\)</span></p></li>
<li><p>and the value <span class="math notranslate nohighlight">\(g(x',y')\)</span>, where <span class="math notranslate nohighlight">\(g(x',y')\)</span> is a circular Gaussian window, centered at the keypoint <span class="math notranslate nohighlight">\((x,y)\)</span>. The standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span> is <span class="math notranslate nohighlight">\(1.5\)</span> times the scale of the keypoint.</p></li>
</ul>
</li>
</ol>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/histogramOfGradients.jpg" style="width:650px" align="center">
</figure>
</div>
<div class="section" id="assign-orientations-to-keypoints">
<h4>Assign Orientations to Keypoints<a class="headerlink" href="#assign-orientations-to-keypoints" title="Permalink to this headline">¶</a></h4>
<p>For each histogram the orientations with the</p>
<ul class="simple">
<li><p>the peak value</p></li>
<li><p>values <span class="math notranslate nohighlight">\(&gt;80\%\)</span> of the histogram’s peak</p></li>
</ul>
<p>constitute keypoint orientations. Thus for a given keypoint location, <span class="math notranslate nohighlight">\(&gt;1\)</span> keypoints can be generated. In the average <span class="math notranslate nohighlight">\(15\%\)</span> of all keypoints have multiple orientations. To provide better accuracy of the keypoint orientations quadratic interpolation w.r.t. the <span class="math notranslate nohighlight">\(3\)</span> closest histogram values is applied.</p>
</div>
</div>
<div class="section" id="keypoint-descriptor">
<h3>Keypoint descriptor<a class="headerlink" href="#keypoint-descriptor" title="Permalink to this headline">¶</a></h3>
<p>For each of the remaining keypoints a descriptor, i.e. a numeric vector of predefined length, is calculated as follows:</p>
<ol class="simple">
<li><p>Determine <strong>gradient magnitude and orientations</strong> at all points around the keypoint as described above. In the pictures below two options for the size of the keypoint neighbourhood are shown. In the topmost image the <span class="math notranslate nohighlight">\(8x8\)</span> pixels around the keypoint constitute the neighbourhood. In the second image the neighbourhood consists of the <span class="math notranslate nohighlight">\(16x16\)</span> pixels around the keypoint. This neighbourhood-size is a parameter, which can be configured. As shown below a large neighbourhood-yields a longer feature-descriptor.</p></li>
<li><p>In order to <strong>achieve orientation invariance</strong>, the coordinates of the descriptor and the gradient orientations are rotated relative to the keypoint orientation.</p></li>
<li><p><strong>A Gaussian weighting</strong> function with <span class="math notranslate nohighlight">\(\sigma\)</span> equal to one half the width of the descriptor window is used to assign a weight to the magnitude of each sample point. Thus the influence of sample points at the window boundary is reduced and the keypoint descriptor is less sensitive to small shifts of the window.</p></li>
<li><p>For each of the <strong><span class="math notranslate nohighlight">\((4x4)\)</span>-subregions a single histogram</strong> is created.</p></li>
<li><p>Each histogram has 8 bins (directions).</p></li>
<li><p>For a neighbourhood-size of <span class="math notranslate nohighlight">\((8x8)\)</span> (topmost image below), there are <span class="math notranslate nohighlight">\(4\)</span> subregions. Each subregion is described by a histogram of 8 bins and the <strong>final keypoint descriptor is a vector of <span class="math notranslate nohighlight">\(4 \cdot 8 = 32\)</span> elements</strong>. For a neighbourhood-size of <span class="math notranslate nohighlight">\((16x16)\)</span> (second image) there are <span class="math notranslate nohighlight">\(16\)</span> subregions. Each subregion is described by a histogram of 8 bins and the <strong>final keypoint descriptor is a vector of <span class="math notranslate nohighlight">\(16 \cdot 8 = 128\)</span> elements</strong>.</p></li>
</ol>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/SIFTdescriptor2x2.PNG" style="width:650px" align="center">
    <figcaption>(2x2)-Regions around the keypoint, yielding a descriptor of length 8*2*2=32</figcaption>
</figure>
<figure align="center">
<img src="https://maucher.home.hdm-stuttgart.de/Pics/SIFTdescriptor4x4.jpg" style="width:650px" align="center">
    <figcaption>(4x4)-Regions around the keypoint, yielding a descriptor of length 8*4*4=128</figcaption>
</figure>
<p>Note that the descriptor may change abruptly if a sample shifts smoothly from</p>
<ul class="simple">
<li><p>one histogram to another</p></li>
<li><p>one orientation bin to another</p></li>
</ul>
<p>In order to avoid such abrupt changes, <a class="reference external" href="https://en.wikipedia.org/wiki/Trilinear_interpolation">trilinear interpolation</a>, which distributes the value of each gradient sample into adjacent histogram bins, is usually applied.</p>
<p>SIFT descriptors are not only translation-, rotation- and scale-invariant. They are also robust w.r.t. illumination (contrast and brightness) changes. A <strong>contrast change</strong> is basically a multiplication of all pixel-values with a constant factor. Then the gradients are multiplied by the same constant, but this constant is cancelled out in the case that the histograms are normalized to unit length. A <strong>brightness change</strong> is an addition of a constant to all pixel values. An additonal constant does not have any impact on the gradients, therefore the descriptor is also invariant w.r.t. a homogenous brightness variation.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./features"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="probRecognition.html" title="previous page">Example: Naive Bayes Object Recognition</a>
    <a class='right-next' id="next-link" href="harrisCornerDetection.html" title="next page">Example: Harris-Förstner Corner Detection</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Prof. Dr. Johannes Maucher<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.3da636dd464baa7582d2.js"></script>


    
  </body>
</html>