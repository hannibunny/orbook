
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Object Recognition &#8212; Object Recognition Lecture</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.5f77b4aec8189eecf79907ce328c390d.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Convolutional Neural Networks" href="../deeplearning/ConvolutionNeuralNetworks.html" />
    <link rel="prev" title="HOG-based Pedestrian Detection" href="../features/HOGpedestrianDetection.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/hdmlogomed.jpg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Object Recognition Lecture</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../intro.html">
   Intro and Overview Object Recognition Lecture
  </a>
 </li>
</ul>
<p>
 <span class="caption-text">
  Image Processing
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../preprocessing/01accessImage.html">
   Basic Image Access Operations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../preprocessing/02filtering.html">
   Basic Filter Operations
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../preprocessing/04gaussianDerivatives.html">
   Gaussian Filter and Derivatives of Gaussian
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../preprocessing/03LowPassFilter.html">
   Rectangular- and Gaussian Low Pass Filtering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../preprocessing/06GaussianNoiseReduction.html">
   Noise Suppression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../preprocessing/05GaussianLowPassFilter.html">
   Gaussian and Difference of Gaussian Pyramid
  </a>
 </li>
</ul>
<p>
 <span class="caption-text">
  Features
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../features/globalDescriptors.html">
   Global Image Features
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../features/similarityMetrics.html">
   Similarity Measures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../features/ImageRetrieval.html">
   Histogram-based Image Retrieval
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../features/multiReceptiveFields.html">
   Multidimensional Receptive Field Histograms
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../features/naiveBayesHistogram.html">
   Histogram-based Naive Bayes Recognition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../features/probRecognition.html">
   Example: Naive Bayes Object Recognition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../features/localFeatures.html">
   Local Image Features
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../features/harrisCornerDetection.html">
   Example: Harris-Förstner Corner Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../features/siftDescriptorCV2.html">
   Example: Create SIFT Descriptors with openCV
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../features/HoGfeatures.html">
   Histogram of Oriented Gradients: Step-by-Step
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../features/HOGpedestrianDetection.html">
   HOG-based Pedestrian Detection
  </a>
 </li>
</ul>
<p>
 <span class="caption-text">
  Object Recognition
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Object Recognition
  </a>
 </li>
</ul>
<p>
 <span class="caption-text">
  Deep Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../deeplearning/ConvolutionNeuralNetworks.html">
   Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../deeplearning/convolutionDemos.html">
   Animations of Convolution and Deconvolution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../deeplearning/cnns.html">
   Convolutional Neural Networks for Object Recognition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../deeplearning/detection.html">
   Object Detection
  </a>
 </li>
</ul>
<p>
 <span class="caption-text">
  Face Detection and Recognition
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../face/faceDetection.html">
   Face Detection
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../face/faceRecognition.html">
   Face Recognition using FaceNet
  </a>
 </li>
</ul>
<p>
 <span class="caption-text">
  Pose Estimation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../poseEstimation/Pose_Estimation.html">
   Multi-Person 2D Pose Estimation using Part Affinity Fields
  </a>
 </li>
</ul>
<p>
 <span class="caption-text">
  References
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../referenceSection.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/recognition/objectrecognition.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bags-of-visual-words">
   Bags of Visual Words
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-origin-bag-of-word-document-model-in-nlp">
     The origin: Bag of Word Document Model in NLP
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#from-bow-to-bag-of-visual-words">
     From BoW to Bag of Visual Words
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#k-means-clustering-for-determining-the-visual-words">
       k-means clustering for determining the visual words
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bag-of-visual-words-based-image-classification">
     Bag-Of-Visual-Words based image classification
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bag-of-visual-words-design-choices">
     Bag-Of-Visual-Words Design Choices
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#how-to-sample-local-descriptors-from-the-image">
       How to sample local descriptors from the image?
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#data-for-training-the-visual-vocabulary">
       Data for Training the Visual Vocabulary?
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#number-of-visual-words-and-how-to-count-words-in-the-matrix">
       Number of Visual Words and how to count words in the matrix?
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#algorithms-for-training-and-encoding">
       Algorithms for Training and Encoding?
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bag-of-visual-words-summary-of-pros-and-cons">
     Bag of Visual Words: Summary of Pros and Cons
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#spatial-pyramid-matching">
   Spatial Pyramid Matching
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pyramid-match-kernel">
     Pyramid Match Kernel
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#combine-pmk-and-bovw-to-spatial-pyramid-matching">
     Combine PMK and BoVW to Spatial Pyramid Matching
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#applying-spm-for-image-classification">
     Applying SPM for Image Classification
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sampling-of-local-descriptors">
       Sampling of local descriptors
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#histogram-normalization">
       Histogram Normalization
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#apply-spm-as-kernel-in-svm-classifier">
       Apply SPM as kernel in SVM classifier
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#obtained-results">
       Obtained Results
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#intermediate-summary-on-spatial-pyramid-matching">
       Intermediate Summary on Spatial Pyramid Matching
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sparse-coding">
     Sparse Coding
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#new-notation-for-describing-k-means">
       New notation for describing k-means
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#from-k-means-to-sparse-coding">
       From k-means to sparse-coding
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#replace-non-linear-by-linear-svm">
     Replace non-linear by linear SVM
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#sparse-coding-for-generating-mid-level-features">
       Sparse Coding for generating mid level features
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#maxpooling">
       Maxpooling
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#integrate-linear-svm">
       Integrate Linear SVM
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#performance-of-the-linear-spatial-pyramid-matching-approach">
       Performance of the linear Spatial Pyramid Matching approach
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <section id="object-recognition">
<h1>Object Recognition<a class="headerlink" href="#object-recognition" title="Permalink to this headline">¶</a></h1>
<p>In this subsection conventional methods for object recognition are described. What is <em>conventional</em>? And which type of <em>object recognition</em>? In the context of this section <em>conventional</em> means <em>no deeplearning</em> and <em>object recognition</em> refers to the task, where given an image, the category of the object, which is predominant in this image must be determined.
Even though <em>deep learning</em> approaches are excluded in this section, the evolution path of conventional techniques, as described here, actually lead to the development of deep neural networks for object recognition. Hence, understanding the evolution of conventional techniques also helps to understand the ideas and concepts of deep learning. Deep neural networks for object recognition are subject of all of the following sections.</p>
<section id="bags-of-visual-words">
<h2>Bags of Visual Words<a class="headerlink" href="#bags-of-visual-words" title="Permalink to this headline">¶</a></h2>
<p>The goal of <strong>feature engineering</strong> is to find for a given input an informative feature representation, which can be passed as a numeric vector to a machine learning algorithm, such that this algorithm is able to learn a good model, e.g. an accurate image-classifier. One important and popular approach for this essential question is the so called <strong>Bag of Visual Words</strong> feature representation. This approach is actually borrowed from the field of <em>Natural Language Processing</em>, where <em>Bag of Word</em>-representations are a standard representations for texts. In this subsection we first describe the <em>Bag of Word</em>-model as it is applied in NLP. Then this model is transformed to the concept <em>Bag of Visual Words</em> for Object Recognition.</p>
<section id="the-origin-bag-of-word-document-model-in-nlp">
<h3>The origin: Bag of Word Document Model in NLP<a class="headerlink" href="#the-origin-bag-of-word-document-model-in-nlp" title="Permalink to this headline">¶</a></h3>
<p>In NLP documents are typically described as vectors, which count the occurence of each word in the document. These vectors are called <strong>Bag of Words (BoW)</strong>. The BoW-representations of all text in a given corpus constitute the Document-Word matrix. Each document belongs to a single row in the matrix and each word, which is present at least once in the entire corpus, belongs to a single column in the matrix. The set of all word in the corpus is called the <em>vocabulary</em>.</p>
<p>For example, assume that there are only 2 documents in the corpus:</p>
<ul class="simple">
<li><p><strong>Document 1:</strong> <em>cars, trucks and motorcyles pollute air</em></p></li>
<li><p><strong>Document 2:</strong> <em>clean air for free</em></p></li>
</ul>
<p>The corresponding document-word-matrix of this corpus is:</p>
<figure class="align-center" id="bow">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/BoW.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/BoW.png" src="https://maucher.home.hdm-stuttgart.de/Pics/BoW.png" style="width: 650px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">Bag-of-Word representation of 2 simple documents</span><a class="headerlink" href="#bow" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="from-bow-to-bag-of-visual-words">
<h3>From BoW to Bag of Visual Words<a class="headerlink" href="#from-bow-to-bag-of-visual-words" title="Permalink to this headline">¶</a></h3>
<p>In contrast to NLP in Object Recognition the input are not texts but images. Moreover, the <strong>input objects are not described by words but by local features</strong>, such as e.g. SIFT features. Hence the <em>Bag of Visual Words</em>-model of an image, contains for each local feature the frequency of the feature in the image. However, this idea must be adapted slightly because there is an important distinction between words in a textcorpus and visual words in a collection of images: <strong>In the text corpus there exists a finite number of different words, but the space of visual words in an image-collection is continuous with an infinite number of possible descriptors</strong> (note that the local descriptors are float-vectors). Hence, the problem is <em>how to quantize the continuous space of local descriptors into a finite discrete set of clusters?</em> The answer is: <em>By applying a clustering-algorithms!</em> Such an algorithm, e.g. k-means clustering, calculates for a given set of vectors and for a user-defined number of clusters <span class="math notranslate nohighlight">\(k\)</span>, a partition into <span class="math notranslate nohighlight">\(k\)</span> cells (=clusters), such that similar vectors are assigned to the same cluster.</p>
<p>The centroids of the calculated <span class="math notranslate nohighlight">\(k\)</span> clusters constitute the set of visual words, also called the <strong>Visual Vocabulary:</strong></p>
<div class="math notranslate nohighlight">
\[
V=\lbrace v_t \rbrace_{t=1}^K
\]</div>
<p>In the Bag-Of-Visual Word matrix, each of the <span class="math notranslate nohighlight">\(k\)</span> columns corresponds to a visual word <span class="math notranslate nohighlight">\(v_t \in V\)</span> and each image in the set ov <span class="math notranslate nohighlight">\(N\)</span> images</p>
<div class="math notranslate nohighlight">
\[
I=\lbrace I_i \rbrace_{i=1}^N
\]</div>
<p>corresponds to a row. The entry <span class="math notranslate nohighlight">\(N(t,i)\)</span> in row <span class="math notranslate nohighlight">\(i\)</span>, column <span class="math notranslate nohighlight">\(t\)</span> of the matrix counts the number of times visual word <span class="math notranslate nohighlight">\(v_t\)</span> occurs in image <span class="math notranslate nohighlight">\(I_i\)</span>. Or more accurate: <span class="math notranslate nohighlight">\(N(t,i)\)</span> is the number of local descriptors in image <span class="math notranslate nohighlight">\(i\)</span>, which are assigned to the <span class="math notranslate nohighlight">\(t.th\)</span> cluster.</p>
<div class="math notranslate nohighlight" id="equation-bovwmatrix">
<span class="eqno">(14)<a class="headerlink" href="#equation-bovwmatrix" title="Permalink to this equation">¶</a></span>\[\begin{split}
\begin{array}{c|ccccc}
 &amp;  v_1  &amp;  v_2  &amp;  v_3  &amp;  \cdots  &amp;  v_K  \\ 
\hline 
 I_1  &amp; 2 &amp; 0 &amp; 1 &amp;   \cdots   &amp; 0 \\ 
 I_2  &amp; 0 &amp; 0 &amp; 3 &amp;  \cdots  &amp; 1 \\ 
 \vdots  &amp;  \vdots  &amp;  \vdots  &amp;  \vdots  &amp;  \vdots  &amp;  \vdots  \\  
 I_N  &amp; 2 &amp; 2 &amp; 0 &amp;  \cdots  &amp; 0 \\ 
\end{array} 
\end{split}\]</div>
<p>Once images are described in this numeric matrix-form, any supervised or unsupervised machine learning algorithm can be applied, e.g. for content based image retrieval (CBIR) or object recognition. The main drawback of this representation is, that spatial information in the image is totally ignored. Bag-of-Visual-Word representation encode information about what type of structures (visual words) are contained in the image, but not where these structures appear.</p>
<figure class="align-center" id="visualwords1">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/visualwordsSchema1.PNG"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/visualwordsSchema1.PNG" src="https://maucher.home.hdm-stuttgart.de/Pics/visualwordsSchema1.PNG" style="width: 650px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">Calculation of visual words: First, from all images of the given collection, the local descriptors (e.g. SIFT) are extracted. These descriptors are points in the d-dimensional space (d=128 in the case of SIFT). The set of all descriptors is passed to a clustering algorithm, which partitions the d-dimensional space into a discrete set of k clusters. The center of a cluster (green points) is just the centroid of all descriptors, which belong to this cluster. Each cluster center constitutes a visual word. The set of all visual words is called visual vocabulary.</span><a class="headerlink" href="#visualwords1" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-center" id="visualwords2">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/visualwordsSchema2.PNG"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/visualwordsSchema2.PNG" src="https://maucher.home.hdm-stuttgart.de/Pics/visualwordsSchema2.PNG" style="width: 650px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">Mapping of local descriptors to visual words: Once the visual words (cluster-centroids) are known, all local descriptors, can be mapped to their closest visual word. After this mapping for each visual word it’s frequency in the given image can be determined and the vector of all these frequencies is the feature vector of the image.</span><a class="headerlink" href="#visualwords2" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The image below is from <span id="pending-xref-1">[<a class="reference internal" href="../referenceSection.html#citation-44">SZ03</a>]</span>.</p>
<figure class="align-center" id="visualwordpatches">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/visualwordsPatches.PNG"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/visualwordsPatches.PNG" src="https://maucher.home.hdm-stuttgart.de/Pics/visualwordsPatches.PNG" style="width: 650px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 4 </span><span class="caption-text">For 4 different visual words, local image-regions are displayed, which are assigned to the same visual word. As can be seen local descriptors, which belong to the same visual word, actually describe similar local regions.</span><a class="headerlink" href="#visualwordpatches" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<section id="k-means-clustering-for-determining-the-visual-words">
<h4>k-means clustering for determining the visual words<a class="headerlink" href="#k-means-clustering-for-determining-the-visual-words" title="Permalink to this headline">¶</a></h4>
<p>K-means clustering is maybe the most popular algorithm of <strong>unsupervised machine-learning</strong>. Due to it’s simplicity and low complexity it can be found in many applications. One of it’s main application categories is <strong>quantization</strong>. E.g. statistical quantizations of analog signals such as audio. In the context of this section k-means is applied for quantizing the continuous space of d-dimensional local descriptors.</p>
<p>The algorithm’s flow chart is depicted below:</p>
<figure class="align-center" id="kmeans">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/kmeansClustEnglish.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/kmeansClustEnglish.png" src="https://maucher.home.hdm-stuttgart.de/Pics/kmeansClustEnglish.png" style="width: 350px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 5 </span><span class="caption-text">Flow chart of k-means clustering</span><a class="headerlink" href="#kmeans" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><strong>k-means clustering:</strong></p>
<ol>
<li><p>First, the number of clusters <span class="math notranslate nohighlight">\(k\)</span> must be defined by the user</p></li>
<li><p>Then <span class="math notranslate nohighlight">\(k\)</span> initial cluster centers <span class="math notranslate nohighlight">\(\mathbf{v}_i\)</span> are randomly placed into the d-dimensional space</p></li>
<li><p>Next, each of the given vectors <span class="math notranslate nohighlight">\(\mathbf{x}_p \in T\)</span> is assigned to it’s closest center-vector <span class="math notranslate nohighlight">\(\mathbf{v}_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[
	\left\|\mathbf{x}_p-\mathbf{v}_i \right\| = \min\limits_{j \in 1 \ldots k } \left\|\mathbf{x}_p-\mathbf{v}_j \right\|,
	\]</div>
<p>where <span class="math notranslate nohighlight">\(\left\| \mathbf{x}-\mathbf{v} \right\|\)</span> is the distance between <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{v}\)</span>.</p>
</li>
<li><p>After this assignment for each cluster <span class="math notranslate nohighlight">\(C_i\)</span> the new cluster centroid <span class="math notranslate nohighlight">\(\mathbf{v}_i\)</span> is calculated as follows:</p>
<div class="math notranslate nohighlight">
\[
	\mathbf{v}_i = \frac{1}{n_i} \sum\limits_{\forall \mathbf{x}_p \in C_i} \mathbf{x}_p,
	\]</div>
<p>where <span class="math notranslate nohighlight">\(n_i\)</span> is the number of vectors that are assigned to <span class="math notranslate nohighlight">\(C_i\)</span>.</p>
</li>
<li><p>Continue with step 3 until the cluster-centers remain at a stable position.</p></li>
</ol>
<p>This algorithm guarantees, that it’s result is a local minimum of the <strong>Reconstruction Error</strong>, which is defined by</p>
<div class="math notranslate nohighlight" id="equation-recerror">
<span class="eqno">(15)<a class="headerlink" href="#equation-recerror" title="Permalink to this equation">¶</a></span>\[
E=\sum\limits_{i=1}^K \sum\limits_{\forall \mathbf{x}_p \in C_i} \left\|\mathbf{x}_p-\mathbf{v}_i \right\|
\]</div>
<p>A nice demonstration of the k-means clustering algorithm is given here <a class="reference external" href="https://user.ceng.metu.edu.tr/~akifakkus/courses/ceng574/k-means/">k-means clustering demo</a>.</p>
<p>Drawbacks of the algorithms are:</p>
<ul class="simple">
<li><p>the number of clusters <span class="math notranslate nohighlight">\(k\)</span> must be definied in advance by the user</p></li>
<li><p>the algorithm terminates in a local minimum of the reconstruction error and different runs of the algorithms usually terminate in different local minimas, since the random placement of the initial cluster-centers differ.</p></li>
<li><p>as usual in unsupervised learning the quality of the result can hardly be assesed. Quality can be assesed implicetly, if the cluster-result is applied in the context of a downstream supervised learning task, e.g. for image classification.</p></li>
</ul>
<p>Concerning these problems, the authors of <em>Visual categorization with bags of keypoints</em> (<span id="pending-xref-2">[<a class="reference internal" href="../referenceSection.html#citation-2">CDF+04</a>]</span>) propose:</p>
<ol class="simple">
<li><p>Iterate over a wide range of values for <span class="math notranslate nohighlight">\(k\)</span></p></li>
<li><p>For each <span class="math notranslate nohighlight">\(k\)</span> restart k-means 10 times to obtain 10 different vocabularies</p></li>
<li><p>For each <span class="math notranslate nohighlight">\(k\)</span> and each vocabulary train and test a multiclass classifier (e.g. Naive Bayes as will be described below)</p></li>
<li><p>Select <span class="math notranslate nohighlight">\(k\)</span> and vocabulary, which yields lowest classification error rate.</p></li>
</ol>
</section>
</section>
<section id="bag-of-visual-words-based-image-classification">
<h3>Bag-Of-Visual-Words based image classification<a class="headerlink" href="#bag-of-visual-words-based-image-classification" title="Permalink to this headline">¶</a></h3>
<p>As already mentioned, the Bag of Visual Words matrix, as given in equation <a class="reference internal" href="#equation-bovwmatrix">(14)</a>, can be passed as input to any machine learning algorithm. In the original work on visual words (<span id="pending-xref-3">[<a class="reference internal" href="../referenceSection.html#citation-2">CDF+04</a>]</span>), the authors applied a Naive Bayes Classifier. This approach and the results obtained in <span id="pending-xref-4">[<a class="reference internal" href="../referenceSection.html#citation-2">CDF+04</a>]</span> are described in this subsection. Note, that the Naive Bayes classifier has already been described in <a class="reference internal" href="../features/naiveBayesHistogram.html"><span class="doc std std-doc">Histogram-based Naive Bayes Object Recognition</span></a>.</p>
<p><strong>Inference:</strong></p>
<p>Assume that a query image <span class="math notranslate nohighlight">\(I_q\)</span> shall be assigned to one of <span class="math notranslate nohighlight">\(L\)</span> object categories <span class="math notranslate nohighlight">\(C_1,C_2,\ldots,C_L\)</span>. For this a trained Naive Bayes Classifier calculates the <strong>a-posteriori probability</strong></p>
<div class="math notranslate nohighlight" id="equation-eq-nbfull">
<span class="eqno">(16)<a class="headerlink" href="#equation-eq-nbfull" title="Permalink to this equation">¶</a></span>\[
P(C_j|I_q) = \frac{\prod_{t=1}^K P(v_t|C_j)^{N(t,q)} P(C_j)}{P(I_q)}
\]</div>
<p>for all classes <span class="math notranslate nohighlight">\(C_j\)</span>. The class which maximizes this expression is the most probable category.</p>
<p>The class which maximizes <a class="reference internal" href="#equation-eq-nbfull">(16)</a> is the same as the class which maximizes</p>
<div class="math notranslate nohighlight" id="equation-eq-nbred">
<span class="eqno">(17)<a class="headerlink" href="#equation-eq-nbred" title="Permalink to this equation">¶</a></span>\[
P^*(C_j|I_q) = \prod_{t=1}^K P(v_t|C_j)^{N(t,q)} P(C_j)
\]</div>
<p>because the denominator in <a class="reference internal" href="#equation-eq-nbfull">(16)</a> is independent of the class.</p>
<p><strong>Training:</strong></p>
<p>In order to calculate <a class="reference internal" href="#equation-eq-nbred">(17)</a> for all classes, the terms on the right hand side of this equation must be estimated for all classes and all visual words from a given set of <span class="math notranslate nohighlight">\(N\)</span> training samples (pairs of images and corresponding class-label).</p>
<p>The <em>a-priori probabilities</em> of the classes are estimated by</p>
<div class="math notranslate nohighlight">
\[
P(C_j)=\frac{|C_j|}{N},
\]</div>
<p>for all classes <span class="math notranslate nohighlight">\(C_1,C_2,\ldots,C_L\)</span>.</p>
<p>For all visual words <span class="math notranslate nohighlight">\(v_t\)</span> and all classes <span class="math notranslate nohighlight">\(C_j\)</span> the <strong>likelihood</strong> is estimated by</p>
<div class="math notranslate nohighlight">
\[
P(v_t|C_j)=\frac{\sum\limits_{I_i \in C_j}N(t,i)}{\sum\limits_{s=1}^K \sum\limits_{I_i \in C_j}N(s,i)}
\]</div>
<p>In order to avoid likelihoods of value <span class="math notranslate nohighlight">\(0\)</span> <strong>Laplace Smoothing</strong> is applied instead of the equation above:</p>
<div class="math notranslate nohighlight">
\[
P(v_t|C_j)=\frac{1+\sum\limits_{I_i \in C_j}N(t,i)}{K+\sum\limits_{s=1}^K \sum\limits_{I_i \in C_j}N(s,i)}
\]</div>
<p>In <span id="pending-xref-5">[<a class="reference internal" href="../referenceSection.html#citation-2">CDF+04</a>]</span> the authors applied a dataset of 1776 images of 7 different classes. 700 images thereof have been applied for test, the remaining for training. Below for each of the 7 classes a few examples are shown:</p>
<figure class="align-center" id="datasetczurka">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/csurkaDataset.PNG"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/csurkaDataset.PNG" src="https://maucher.home.hdm-stuttgart.de/Pics/csurkaDataset.PNG" style="width: 650px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 6 </span><span class="caption-text">Some representatives for each of the 7 classes. As can be seen the intraclass-variance due to pose, view-point, scale, color, etc. is quite high. Moreover, there is a significant amount of background clutter</span><a class="headerlink" href="#datasetczurka" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The resolution of the images varies from 0.3 to 2.1 megapixels. Only the luminance channel (greyscale-image) of the color-images has been applied for the classification task.</p>
<p>The confusion matrix and the the mean-rank<a class="footnote-reference brackets" href="#footnote1" id="footnote-reference-1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a> on the test-data is depicted in the image below:</p>
<figure class="align-center" id="confmat">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/csurkaNaiveBayesResult.PNG"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/csurkaNaiveBayesResult.PNG" src="https://maucher.home.hdm-stuttgart.de/Pics/csurkaNaiveBayesResult.PNG" style="width: 350px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 7 </span><span class="caption-text">Confusion matrix and mean rank on test data for Naive Bayes classification. k=1000 visual words have been applied</span><a class="headerlink" href="#confmat" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-center" id="confmatsvm">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/csurkaSVMResult.PNG"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/csurkaSVMResult.PNG" src="https://maucher.home.hdm-stuttgart.de/Pics/csurkaSVMResult.PNG" style="width: 350px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 8 </span><span class="caption-text">Confusion matrix and mean rank on test data for SVM classification. k=1000 visual words have been applied</span><a class="headerlink" href="#confmatsvm" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>By applying a SVM the error rate dropped from <span class="math notranslate nohighlight">\(0.28\)</span> in the case of Naive Bayes to <span class="math notranslate nohighlight">\(0.15\)</span>.</p>
<p>Concerning the question on <em>Which size of Visual Vocabulary <span class="math notranslate nohighlight">\(k\)</span> is the best?</em> the following chart shows the error-rate decrease with increasing <span class="math notranslate nohighlight">\(k\)</span> in the Naive Bayes classifier option (from <span id="pending-xref-6">[<a class="reference internal" href="../referenceSection.html#citation-2">CDF+04</a>]</span>).</p>
<figure class="align-center" id="errorratevsk">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/bowBestK.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/bowBestK.png" src="https://maucher.home.hdm-stuttgart.de/Pics/bowBestK.png" style="width: 350px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 9 </span><span class="caption-text">Reduction of error rate with increasing k in the Naive Bayes classification case</span><a class="headerlink" href="#errorratevsk" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="bag-of-visual-words-design-choices">
<h3>Bag-Of-Visual-Words Design Choices<a class="headerlink" href="#bag-of-visual-words-design-choices" title="Permalink to this headline">¶</a></h3>
<p>The work of Czurka et al (<span id="pending-xref-7">[<a class="reference internal" href="../referenceSection.html#citation-2">CDF+04</a>]</span>) introduced the idea of visual words and demonstrated it’s potential. Moreover, it motivated a wide range of research questions in it’s context. These questions are sketched in the following subsections:</p>
<section id="how-to-sample-local-descriptors-from-the-image">
<h4>How to sample local descriptors from the image?<a class="headerlink" href="#how-to-sample-local-descriptors-from-the-image" title="Permalink to this headline">¶</a></h4>
<p>In <a class="reference internal" href="../features/localFeatures.html"><span class="doc std std-doc">Local Fetures</span></a> SIFT has been introduced. There, only the option that these descriptors are sampled at the image’s keypoint (sparse sampling) has been mentioned. However, keypoint detection on one hand and describing a local region on the other hand are decoupled. In fact, as the image below shows, local descriptors can also be extracted at all points of a regular grid (dense sampling) or even at randomly determined points (random sampling).</p>
<figure class="align-center" id="samplingoptions">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/localFeatureSampling.PNG"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/localFeatureSampling.PNG" src="https://maucher.home.hdm-stuttgart.de/Pics/localFeatureSampling.PNG" style="width: 550px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 10 </span><span class="caption-text">Sparse sampling (left), dense sampling (center) and random sampling (right)</span><a class="headerlink" href="#samplingoptions" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Random sampling is frequently the option of choice for <em>scene-detection</em>. The advantage of dense, and random sampling is that no effort must be spent for finding the keypoints.</p>
</section>
<section id="data-for-training-the-visual-vocabulary">
<h4>Data for Training the Visual Vocabulary?<a class="headerlink" href="#data-for-training-the-visual-vocabulary" title="Permalink to this headline">¶</a></h4>
<p>Visual words must be learned, e.g. by applying the k-means algorithm. Which image-datasets shall be applied for training? The best results are obtained, if the same training data is applied as for the downstream task (e.g. image classification). However, it is also possible to learn a set of generally applicable visual words, which can then be applied for different downstream tasks with different training data. This approach may be helpful, if only few labeled data is available for the downstream task - too few to learn a good visual vocabulary.</p>
</section>
<section id="number-of-visual-words-and-how-to-count-words-in-the-matrix">
<h4>Number of Visual Words and how to count words in the matrix?<a class="headerlink" href="#number-of-visual-words-and-how-to-count-words-in-the-matrix" title="Permalink to this headline">¶</a></h4>
<p>Up to now, we pretended that the entries in the Bag of Visual Words matrix are the numbers of local descriptors in the image of the current row, which are assigned to the visual word of the current column. However, using the visual-word frequencies is only one option. In BoWs in NLP an alternative to word-frequencies is the so called <strong>tf-idf</strong> (term frequency -inverse document frequency), where</p>
<ul>
<li><p>term frequency <span class="math notranslate nohighlight">\(tf_{i,j}\)</span> counts how often word <span class="math notranslate nohighlight">\(i\)</span> appears in document <span class="math notranslate nohighlight">\(j\)</span>, and</p></li>
<li><p>inverse document frequency <span class="math notranslate nohighlight">\(idf_i\)</span> is the logarithm of inverse ratio of documents (images) that contain (visual) word <span class="math notranslate nohighlight">\(i\)</span>:</p>
<div class="math notranslate nohighlight">
\[
	idf_i=\log\left(\frac{N}{n_i}\right).
	\]</div>
<p>Here, <span class="math notranslate nohighlight">\(N\)</span> is the number of documents, and <span class="math notranslate nohighlight">\(n_i\)</span> is the number of documents, that contain word <span class="math notranslate nohighlight">\(i\)</span>.</p>
</li>
<li><p>term frequency -inverse document frequency is then defined as follows:</p>
<div class="math notranslate nohighlight">
\[
	tf\mbox{-}idf_{i,j}=tf_{i,j} \cdot idf_i
	\]</div>
</li>
</ul>
<p>The idea of tf-idf is that words, which occur in many documents, are less informative and are therefore weighted with a small value, whereas for rarely appearing words the weight <span class="math notranslate nohighlight">\(idf_i\)</span> is higher. For example in the <em>Video-Google</em>-work <span id="pending-xref-8">[<a class="reference internal" href="../referenceSection.html#citation-44">SZ03</a>]</span>, the three matrix-entry options</p>
<ul class="simple">
<li><p>binary count (1 if visual word appears at least once in the image, 0 otherwise)</p></li>
<li><p>term-frequency count</p></li>
<li><p>term frequency -inverse document frequency</p></li>
</ul>
<p>have been compared. The best results have been obtained by tf-idf, the worst by the binary count.</p>
<p>The problem on the size of the visual vocabulary, i.e. the number of different clusters <span class="math notranslate nohighlight">\(k\)</span>, has already been mentioned above. As <a class="reference internal" href="#errorratevsk"><span class="std std-numref">Fig. 9</span></a> shows, in general the error-rate of the downstream task (classification) decreases, with an increasing number of visual words. However, the error-rate-decrease gets smaller in the range of large <span class="math notranslate nohighlight">\(k\)</span> - values. It is also important to be aware, that this plot has been drawn for a specific task and a specific dataset. The question on the number of viusal words must be answered individually for each data set and each task.</p>
</section>
<section id="algorithms-for-training-and-encoding">
<h4>Algorithms for Training and Encoding?<a class="headerlink" href="#algorithms-for-training-and-encoding" title="Permalink to this headline">¶</a></h4>
<p>In the context of visual vocabularies the two stages <em>training</em> and <em>encoding</em> must be distinguished. <strong>Training</strong> of a visual vocabulary means to apply a clustering algorithm to the given training-data in order to determine the set of visual words. <strong>Encoding</strong> means to apply the knwon visual vocabulary in the sense, that for new local-descriptors, the corresponding visual word is determined. Actually, training and encoding are decoupled, i.e. the method, used to encode is independent of the method applied for training. One interesting alternative for training visual vocabularies has been introduced in <span id="pending-xref-9">[<a class="reference internal" href="../referenceSection.html#citation-37">NS06</a>]</span>. In contrast to the standard k-means algorithm, this approach applies a hierarchical k-means clustering, which generates not a flat set of visual words, but a <strong>vocabulary-tree.</strong> The advantage of such an hierarchically ordered vocabulary is, that encoding, i.e. the assignment of new vectors to visual words, is much faster in this structure. As sketched in <a class="reference internal" href="#hierclusttraining"><span class="std std-numref">Fig. 11</span></a> the algorithm first partitions the entire d-dimensional space into a small number (= branching factor <span class="math notranslate nohighlight">\(k\)</span>) different subregions. In the next iteration each subregion is again partitioned into <span class="math notranslate nohighlight">\(k\)</span> subregions and so on. This process is repeated until the maximum depth <span class="math notranslate nohighlight">\(L\)</span> of the tree is reached.</p>
<p><strong>Training: Hierarchical k-means clustering:</strong></p>
<ol class="simple">
<li><p>Initialisation: Define branching-factor <span class="math notranslate nohighlight">\(k\)</span> and maximum depth <span class="math notranslate nohighlight">\(L\)</span>. Set current depth <span class="math notranslate nohighlight">\(l=1\)</span>.</p></li>
<li><p>Apply k-means algorithm to training-set in order to determine k different clusters in depth <span class="math notranslate nohighlight">\(l=1\)</span>.</p></li>
<li><p>Determine for each subregion in depth <span class="math notranslate nohighlight">\(l\)</span> the subsetset of training-data, which is assigned to this subregion.</p></li>
<li><p>Apply k-means algorithm to each of the subregions in depth <span class="math notranslate nohighlight">\(l\)</span>, by applying the subregion-specific training-subset. In this way new subregions for depth <span class="math notranslate nohighlight">\(l+1\)</span> are generated. The number of subregions in depth <span class="math notranslate nohighlight">\(l+1\)</span> is <span class="math notranslate nohighlight">\(k\)</span> times the number of subregions in depth <span class="math notranslate nohighlight">\(l\)</span>.</p></li>
<li><p>Set <span class="math notranslate nohighlight">\(l := l+1\)</span> and continue with step 3 until <span class="math notranslate nohighlight">\(l=L\)</span></p></li>
</ol>
<figure class="align-center" id="hierclusttraining">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/vocabTree1.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/vocabTree1.png" src="https://maucher.home.hdm-stuttgart.de/Pics/vocabTree1.png" style="width: 350px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 11 </span><span class="caption-text">Training: Hierarchical application of <span class="math notranslate nohighlight">\(k\)</span>-means clustering, for branching factor <span class="math notranslate nohighlight">\(k=3\)</span>.</span><a class="headerlink" href="#hierclusttraining" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><strong>Encoding:</strong>
In the encoding phase a new vector <span class="math notranslate nohighlight">\(p\)</span> (point) must be assigned to it’s closest cluster-center (visual word). In the hierearchical tree of visual words, in the first iteration the distance between <span class="math notranslate nohighlight">\(p\)</span> and <span class="math notranslate nohighlight">\(k\)</span> cluster-centers must be determined and the closest is selected. In the next iteration again only <span class="math notranslate nohighlight">\(k\)</span> distances must be determined and compared- the distances between <span class="math notranslate nohighlight">\(p\)</span> and the <span class="math notranslate nohighlight">\(k\)</span> cluster centers of the subregion, which has been found in the previous iteration. In total, far fewer distances must be calculated and compared, than in the encoding phase w.r.t. a flat set of visual words.</p>
<figure class="align-center" id="hierclustencoding">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/vocabTreeRecognition.PNG"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/vocabTreeRecognition.PNG" src="https://maucher.home.hdm-stuttgart.de/Pics/vocabTreeRecognition.PNG" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 12 </span><span class="caption-text">Encoding: In the visual-word-tree new local descriptors must be evaluated in each iteration only w.r.t. <span class="math notranslate nohighlight">\(k\)</span> different cluster-centers.</span><a class="headerlink" href="#hierclustencoding" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The plot below (source <span id="pending-xref-10">[<a class="reference internal" href="../referenceSection.html#citation-37">NS06</a>]</span>), depicts the performance increase with increasing number of visual words and increasing branching factor <span class="math notranslate nohighlight">\(k\)</span>.</p>
<figure class="align-center" id="hierclustperformance">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/vocabTreePerformance.PNG"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/vocabTreePerformance.PNG" src="https://maucher.home.hdm-stuttgart.de/Pics/vocabTreePerformance.PNG" style="width: 550px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 13 </span><span class="caption-text">Accuracy vs. number of Leaf Nodes and vs. branching factor</span><a class="headerlink" href="#hierclustperformance" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>In <span id="pending-xref-11">[<a class="reference internal" href="../referenceSection.html#citation-37">NS06</a>]</span> visual-vocabulary-trees have been applied for real-time recognition of CD-covers:</p>
<figure class="align-center" id="hierclusttest">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/vocabTreeTest.PNG"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/vocabTreeTest.PNG" src="https://maucher.home.hdm-stuttgart.de/Pics/vocabTreeTest.PNG" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 14 </span><span class="caption-text">Real-time recognition of CD-covers.</span><a class="headerlink" href="#hierclusttest" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>There are much more options for implementing the training and encoding of visual words. For example instead of k-means or hierarchical k-means other unsupervised learning algorithms, such as e.g. <em>Restricted Boltzman Machines (RBM)</em> are applied. A comparison can be found e.g. in <span id="pending-xref-12">[<a class="reference internal" href="../referenceSection.html#citation-10">CN11</a>]</span>. Further down in this section the concept of <em>sparse coding</em> as an alternative to the <em>nearest-cluster-center-encoding</em> will be introduced.</p>
</section>
</section>
<section id="bag-of-visual-words-summary-of-pros-and-cons">
<h3>Bag of Visual Words: Summary of Pros and Cons<a class="headerlink" href="#bag-of-visual-words-summary-of-pros-and-cons" title="Permalink to this headline">¶</a></h3>
<p>The Bag-of-Visual-Words (BoVW) concept has proven to be a good feature representation for robust object recognition (w.r.t. scale, background clutter, partial occlusion, translation and rotation). However, it suffers from a major drawback: The lack of spatial information. BoVW describe <strong>what</strong> is in the image but not <strong>where</strong>. Depending on the application this may be a bad waste of important information. In the next subsection <em>Spatial Pyramid Matching</em>, the most important approach to integrate spatial information and the idea of BoVW, is described. For better understanding the adaptations and extensions, <a class="reference internal" href="#layersbow"><span class="std std-numref">Fig. 15</span></a> sketches the overall architecture of object recognition classifiers, that apply BoVW.</p>
<figure class="align-center" id="layersbow">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/layerSchemaBoW.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/layerSchemaBoW.png" src="https://maucher.home.hdm-stuttgart.de/Pics/layerSchemaBoW.png" style="width: 550px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 15 </span><span class="caption-text">Given an input image, represented by its pixels, first a set of low level local features are extracted, e.g. SIFT features. Low-level features are vectors in a d-dimensional space. This space is quantized by a clustering algorithm, e.g. k-means. More concrete: Given a training-set of d-dimensional low-level features, the clustering algorithm calculates a set of <span class="math notranslate nohighlight">\(k\)</span> cluster centers, which are the visual words. Once the visual words are known, any low-level feature vector can be assigned to a visual word and the visual words of all low-level features in the image constitute the BoVW-representation of the image. This BovW is also called the mid-level representation of the image. The mid-level representation of the image can be passed to any supervised learning algorithm for classification. This classifier is trained by many pairs of labeled input data.</span><a class="headerlink" href="#layersbow" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
</section>
<section id="spatial-pyramid-matching">
<h2>Spatial Pyramid Matching<a class="headerlink" href="#spatial-pyramid-matching" title="Permalink to this headline">¶</a></h2>
<p><strong>Spatial Pyramid Matching (SPM)</strong> has been introduced in <span id="pending-xref-13">[<a class="reference internal" href="../referenceSection.html#citation-31">LSP06</a>]</span>. It can be considered as a combination of <strong>Pyramid Match Kernels</strong> and <strong>BoVW</strong>. It’s main advantage is that it integrates spatial information into BoVW. The underlying idea is <strong>Subdivide and Disorder</strong>. This concept is described in the image below:</p>
<figure class="align-center" id="subdivide">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/localHist.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/localHist.png" src="https://maucher.home.hdm-stuttgart.de/Pics/localHist.png" style="width: 550px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 16 </span><span class="caption-text">Subdivide and disorder: The image is subdivided into local regions. Within each region an orderless descriptor, e.g. a histogram of color, is obtained. All of these descriptors are concatenated in the order of the local regions to form a long descriptor of the entire image.</span><a class="headerlink" href="#subdivide" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>In this subsection, first the concept of <em>Pyramid Match Kernels (PMK)</em>, as introduced in <span id="pending-xref-14">[<a class="reference internal" href="../referenceSection.html#citation-21">GD07</a>]</span> will be described. PMK alone does not integrate spatial information, but it’s integration in Spatial Pyramid Matching does.</p>
<section id="pyramid-match-kernel">
<h3>Pyramid Match Kernel<a class="headerlink" href="#pyramid-match-kernel" title="Permalink to this headline">¶</a></h3>
<p>Assume that you want to compare two images. <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are the sets of local descriptor vectors of the two images, respectively. The Pyramid Match Kernel measures the correspondence (similarity) of the two descriptor sets <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> as follows:</p>
<ol class="simple">
<li><p>place a sequence of increasingly coarser grids over the <strong>feature space</strong> (not the image space!)</p></li>
<li><p>calculate a weighted sum of the number of matches within cells at each level of resolution.</p></li>
</ol>
<p>This process is sketched in the picture below. However, note that the grids are not applied in image- but in feature-space. The feature space typically consists of 128 dimensions. For the purpose of visualisation the picture below pretends a 2-dimensional feature space.</p>
<figure class="align-center" id="pmkex">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/pyramidMatchConcept.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/pyramidMatchConcept.png" src="https://maucher.home.hdm-stuttgart.de/Pics/pyramidMatchConcept.png" style="width: 650px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 17 </span><span class="caption-text">Example: Pyramid Match Kernel of 2 images: Placing a sequence of increasingly coarser grids over the feature space and calculating a weighted sum of matches within the cells. Blue markers indicate the set of local descriptors <span class="math notranslate nohighlight">\(X\)</span> from the first image, red markers belong to <span class="math notranslate nohighlight">\(Y\)</span>, the set of local descriptors in the second image.</span><a class="headerlink" href="#pmkex" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<ol>
<li><p>Construct a sequence of <span class="math notranslate nohighlight">\(L+1\)</span> grids, such that the grid at level <span class="math notranslate nohighlight">\(\ell \in \lbrace 0,\ldots, L\rbrace\)</span> has <span class="math notranslate nohighlight">\(2^\ell\)</span> cells along each dimension and <span class="math notranslate nohighlight">\(D=2^{d\ell}\)</span> cells in total.</p></li>
<li><p><span class="math notranslate nohighlight">\(H_X^\ell\)</span> and <span class="math notranslate nohighlight">\(H_Y^\ell\)</span> are the histograms of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> at this resolution, so that  <span class="math notranslate nohighlight">\(H_X^\ell(i)\)</span> and <span class="math notranslate nohighlight">\(H_Y^\ell(i)\)</span> are the numbers of points from <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> that fall into the <span class="math notranslate nohighlight">\(i.th\)</span> cell of the grid.</p></li>
<li><p>Then the number of matches at level <span class="math notranslate nohighlight">\(\ell\)</span> is given by the histogram intersection function</p>
<div class="math notranslate nohighlight">
\[
	\mathcal{I}^\ell=\mathcal{I}(H_X^\ell,H_Y^\ell)=\sum\limits_{i=1}^D \min(H_X^\ell(i),H_Y^\ell(i))
	\]</div>
</li>
<li><p>The set of matches at level <span class="math notranslate nohighlight">\(\ell\)</span> includes the set of matches at level <span class="math notranslate nohighlight">\(\ell+1\)</span>. The number of new matches at level <span class="math notranslate nohighlight">\(\ell\)</span> is therefore</p>
<div class="math notranslate nohighlight">
\[
	\mathcal{I}^\ell-\mathcal{I}^{\ell+1}.
	\]</div>
</li>
<li><p>In order to calculate a total score of matches, the matches at finer resolution levels are weighted higher than matches at coarser resolutions.</p></li>
<li><p>The weight associated with level <span class="math notranslate nohighlight">\(\ell\)</span> is</p>
<div class="math notranslate nohighlight">
\[
	\frac{1}{2^{L-\ell}}
	\]</div>
</li>
<li><p>The <strong>Pyramid Match Kernel</strong> is then</p>
<div class="math notranslate nohighlight" id="equation-eq-pmk">
<span class="eqno">(18)<a class="headerlink" href="#equation-eq-pmk" title="Permalink to this equation">¶</a></span>\[\begin{split}
	\kappa^L(X,Y) &amp; = &amp; \mathcal{I}^L+\sum\limits_{\ell=0}^{L-1} \frac{1}{2^{L-\ell}} (\mathcal{I}^\ell-\mathcal{I}^{\ell+1}) \nonumber \\
              &amp; = &amp; \frac{1}{2^{L}} \mathcal{I}^0 +\sum\limits_{\ell=1}^{L} \frac{1}{2^{L-\ell+1}} \mathcal{I}^\ell
	\end{split}\]</div>
</li>
</ol>
<p><strong>Example:</strong></p>
<p>For the descriptor sets <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, depicted in <a class="reference internal" href="#pmkex"><span class="std std-numref">Fig. 17</span></a> the</p>
<figure class="align-center" id="pmkcalc">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/pyramidMatchExample.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/pyramidMatchExample.png" src="https://maucher.home.hdm-stuttgart.de/Pics/pyramidMatchExample.png" style="width: 550px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 18 </span><span class="caption-text">Calculation of the Pyramid Match Kernel according to equation <a class="reference internal" href="#equation-eq-pmk">(18)</a> for the example in figure <a class="reference internal" href="#pmkex"><span class="std std-numref">Fig. 17</span></a>. The blue row contains the historgram-values <span class="math notranslate nohighlight">\(H_X^\ell(i)\)</span> of the first image and the red row contains the historgram-values <span class="math notranslate nohighlight">\(H_Y^\ell(i) \, \mbox{ for } \ell \in \{0,1,2\}\)</span> of the second image. The black row contains the number of matches in the three levels. The resulting PMK is <span class="math notranslate nohighlight">\(\kappa^L(X,Y)=4.75\)</span>.</span><a class="headerlink" href="#pmkcalc" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="combine-pmk-and-bovw-to-spatial-pyramid-matching">
<h3>Combine PMK and BoVW to Spatial Pyramid Matching<a class="headerlink" href="#combine-pmk-and-bovw-to-spatial-pyramid-matching" title="Permalink to this headline">¶</a></h3>
<p>The <strong>Pyramid Match Kernel</strong>, as introduced in <span id="pending-xref-15">[<a class="reference internal" href="../referenceSection.html#citation-21">GD07</a>]</span> and described above, is a totally orderless representation, that discards all spatial information. However, in <span id="pending-xref-16">[<a class="reference internal" href="../referenceSection.html#citation-31">LSP06</a>]</span> Lazebnik et al introduced <strong>Spatial Pyramid Matchin (SPM)</strong>, which combines the concepts of PMK and BoVW to a feature-representation, which contains spatial information. The proposed approach performs <strong>2-dimensional pyramid match kernel in the image space</strong> and <strong>clustering (BoW) in feature space.</strong></p>
<p>Clustering quantizes all features into <strong><span class="math notranslate nohighlight">\(M\)</span> different types</strong> (i.e. the number of clusters is now denoted by <span class="math notranslate nohighlight">\(M\)</span>). In the next step - PMK in image-space - only features of the same type can be matched.</p>
<p>For each type <span class="math notranslate nohighlight">\(m \in \lbrace 1,\ldots, M \rbrace\)</span> two sets <span class="math notranslate nohighlight">\(X_m\)</span> and <span class="math notranslate nohighlight">\(Y_m\)</span>, of 2-dimensional vectors,  each representing the 2-dimensional image-coordinate of a feature of type <span class="math notranslate nohighlight">\(m\)</span>, exists (same as before <span class="math notranslate nohighlight">\(X\)</span> corresponds to one image and <span class="math notranslate nohighlight">\(Y\)</span> to the other image). The <strong>Spatial Pyramid Matching Kernel:</strong> is then calculated as follows:</p>
<div class="math notranslate nohighlight" id="equation-eq-kernel">
<span class="eqno">(19)<a class="headerlink" href="#equation-eq-kernel" title="Permalink to this equation">¶</a></span>\[
K^L(X,Y)=\sum\limits_{m=1}^M \kappa^L(X_m,Y_m)
\]</div>
<p>In this equation <span class="math notranslate nohighlight">\(\kappa^L(X_m,Y_m)\)</span> is calculated as defined in equation <a class="reference internal" href="#equation-eq-pmk">(18)</a> however, now the matching is performed in image-space, not in feature-space.</p>
<p>For <span class="math notranslate nohighlight">\(L=0\)</span> this is the same as if the BoW representations of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are matched.</p>
<p>Since the pyramid match kernel (equation <a class="reference internal" href="#equation-eq-pmk">(18)</a>) is a weighted sum of histogram-intersections and for positive numbers</p>
<div class="math notranslate nohighlight">
\[
w \min(a,b) =  \min(w a,w b),
\]</div>
<p>the spatial pyramid matching <span class="math notranslate nohighlight">\(K^L\)</span> can be represented as a long vector of histogram intersections, formed by concatenating the appropriately weighted histograms of all types <span class="math notranslate nohighlight">\(m\)</span> at all resolutions <span class="math notranslate nohighlight">\(\ell\)</span>. For <span class="math notranslate nohighlight">\(L\)</span> levels and <span class="math notranslate nohighlight">\(M\)</span> types the resulting vector has</p>
<div class="math notranslate nohighlight">
\[
M \sum\limits_{\ell=0}^L 4^{\ell}= \frac{M(4^{L+1}-1)}{3} 
\]</div>
<p>components. The vectors are extremely sparse. Computational complexity of the kernel is linear in the number of the features. Typical values for the parameters are <span class="math notranslate nohighlight">\(M=200\)</span> and <span class="math notranslate nohighlight">\(L=2\)</span>. This yields a vector of length <span class="math notranslate nohighlight">\(4200\)</span>.</p>
<p>The image below sketches a spatial pyramid of a single image. For ease of visualisation, here only <span class="math notranslate nohighlight">\(M=3\)</span> different types of local descriptors are distinguished.</p>
<figure class="align-center" id="spmex">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/spatialPyramidsToyExample.PNG"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/spatialPyramidsToyExample.PNG" src="https://maucher.home.hdm-stuttgart.de/Pics/spatialPyramidsToyExample.PNG" style="width: 550px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 19 </span><span class="caption-text">Toy example for constructing a three-level pyramid. The image has <span class="math notranslate nohighlight">\(M=3\)</span> feature types, indicated by circles, diamonds, and
crosses. The image is subdivided at three different levels of resolution. For each level of resolution and each channel,
the features that fall in each spatial bin are counted <span id="pending-xref-17">[<a class="reference internal" href="../referenceSection.html#citation-31">LSP06</a>]</span>.</span><a class="headerlink" href="#spmex" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="applying-spm-for-image-classification">
<h3>Applying SPM for Image Classification<a class="headerlink" href="#applying-spm-for-image-classification" title="Permalink to this headline">¶</a></h3>
<p>This subsection describes how SPM has been applied for image classification in <span id="pending-xref-18">[<a class="reference internal" href="../referenceSection.html#citation-31">LSP06</a>]</span>.</p>
<section id="sampling-of-local-descriptors">
<h4>Sampling of local descriptors<a class="headerlink" href="#sampling-of-local-descriptors" title="Permalink to this headline">¶</a></h4>
<p>In their experiments the authors of <span id="pending-xref-19">[<a class="reference internal" href="../referenceSection.html#citation-31">LSP06</a>]</span> applied <strong>dense sampling of SIFT descriptors</strong>. I.e. one SIFT descriptor for patches of size (<span class="math notranslate nohighlight">\(16 \times 16\)</span>) pixels has been obtained at all points of a grid with a spacing of <span class="math notranslate nohighlight">\(8\)</span> pixels. This sampling method yields a large number of features per image. Therefore, a <strong>random subset</strong> of features from the original set has been sampled and applied as input for k-means clustering. As mentioned above, dense sampling is particularly recommended for <strong>scene classification</strong> (sparse sampling would not find any keypoints in large homogenous areas like sky or calm water).</p>
</section>
<section id="histogram-normalization">
<h4>Histogram Normalization<a class="headerlink" href="#histogram-normalization" title="Permalink to this headline">¶</a></h4>
<p>The number of local descriptors varies for different images. In order to obtain a robust match kernel, which is independent of the overall number of descriptors per image, the histograms are normalized by the total weight of all features in the image.</p>
</section>
<section id="apply-spm-as-kernel-in-svm-classifier">
<h4>Apply SPM as kernel in SVM classifier<a class="headerlink" href="#apply-spm-as-kernel-in-svm-classifier" title="Permalink to this headline">¶</a></h4>
<p><strong>SVM for binary classification in general:</strong></p>
<p>In general a binary SVM classifier learns a decision function</p>
<div class="math notranslate nohighlight" id="equation-svmbin">
<span class="eqno">(20)<a class="headerlink" href="#equation-svmbin" title="Permalink to this equation">¶</a></span>\[
f(\mathbf{z})=\sum\limits_{i=1}^N \alpha_i y_i \kappa(\mathbf{z},\mathbf{z}_i) +b,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\lbrace(\mathbf{z}_i,y_i)\rbrace_{i=1}^N\)</span> is the set of <span class="math notranslate nohighlight">\(N\)</span> labeled training vectors. The label <span class="math notranslate nohighlight">\(y_i \in \lbrace -1,+1\rbrace\)</span> indicates the class of input <span class="math notranslate nohighlight">\(\mathbf{z}_i\)</span>. Moreover, <span class="math notranslate nohighlight">\(\kappa(\mathbf{z},\mathbf{z}_i)\)</span> is an arbitrary kernel-function. Common kernels are e.g. linear-, polynomial- or radial basis function - kernel. The coefficients <span class="math notranslate nohighlight">\(\alpha_i\)</span> and the bias <span class="math notranslate nohighlight">\(b\)</span> are learned in the training phase. In the inference phase for a new vector <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> the value <span class="math notranslate nohighlight">\(f(\mathbf{z})\)</span> is calculated according to <a class="reference internal" href="#equation-svmbin">(20)</a>. If this value is positive than <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> is assigned to the class labeled by <span class="math notranslate nohighlight">\(y=+1\)</span>. For <span class="math notranslate nohighlight">\(f(\mathbf{z})\leq 0\)</span> the other class is assigned.</p>
<p><strong>SVM for multi-class classification in general:</strong></p>
<p>Multi-class SVM is applied according to the <strong>one-versus-all-rule</strong>: For each class a single binary SVM-classifier is learned, which separates instances of the respective class (label <span class="math notranslate nohighlight">\(y=+1\)</span>) from the rest (label <span class="math notranslate nohighlight">\(y=-1)\)</span>.</p>
<p><strong>SVM with SPM kernel:</strong></p>
<p>In <span id="pending-xref-20">[<a class="reference internal" href="../referenceSection.html#citation-31">LSP06</a>]</span> a multi-class SVM is applied for image classification. The novelity of their approach is, that they apply the Spatial Pyramid Match Kernel (SPM) as kernel-function. From the set of labeled training images for each class a discriminator-function</p>
<div class="math notranslate nohighlight" id="equation-svmspm">
<span class="eqno">(21)<a class="headerlink" href="#equation-svmspm" title="Permalink to this equation">¶</a></span>\[
f(Z)=\sum\limits_{i=1}^N \alpha_i y_i \kappa(Z,Z_i) +b,
\]</div>
<p>where <span class="math notranslate nohighlight">\(\kappa(Z,Z_i)=K^L(Z,Z_i)\)</span> is the <strong>Spatial Pyramid Matching Kernel</strong> as defined in equation <a class="reference internal" href="#equation-eq-kernel">(19)</a> and <span class="math notranslate nohighlight">\(Z\)</span> is the corresponding local-descriptor set of an image.</p>
</section>
<section id="obtained-results">
<h4>Obtained Results<a class="headerlink" href="#obtained-results" title="Permalink to this headline">¶</a></h4>
<p>The authors of <span id="pending-xref-21">[<a class="reference internal" href="../referenceSection.html#citation-31">LSP06</a>]</span> applied and evaluated their approach on three different labeled image datasets:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://figshare.com/articles/dataset/15-Scene_Image_Dataset/7007177">Fifteen Scene Categories</a></p></li>
<li><p><a class="reference external" href="http://www.vision.caltech.edu/Image_Datasets/Caltech101/">Caltech 101</a></p></li>
<li><p><a class="reference external" href="https://www-old.emt.tugraz.at/~pinz/data/GRAZ_02/">GRAZ-02</a></p></li>
</ul>
<p>Some samples from the <em>Fifteen Scene Categories</em> dataset are visualized below:</p>
<figure class="align-center" id="scene">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/sceneCategories.PNG"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/sceneCategories.PNG" src="https://maucher.home.hdm-stuttgart.de/Pics/sceneCategories.PNG" style="width: 650px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 20 </span><span class="caption-text">Samples of the <em>Fifteen Scene Categories</em> dataset</span><a class="headerlink" href="#scene" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The achieved results are summarized in the figure below:</p>
<figure class="align-center" id="lazebnikresults1">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/sceneCategoriesResults.PNG"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/sceneCategoriesResults.PNG" src="https://maucher.home.hdm-stuttgart.de/Pics/sceneCategoriesResults.PNG" style="width: 650px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 21 </span><span class="caption-text">SPM+SVM classification results on the 15 Scene Categories Dataset. <em>Weak Features</em> means sparse sampling, <em>Strong Features</em> refers to dense sampling of SIFT descriptors. Note that the case <span class="math notranslate nohighlight">\(L=0\)</span> is identical with BoVW.</span><a class="headerlink" href="#lazebnikresults1" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-center" id="lazebnikresults2">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/spaCaltech101Results.PNG"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/spaCaltech101Results.PNG" src="https://maucher.home.hdm-stuttgart.de/Pics/spaCaltech101Results.PNG" style="width: 350px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 22 </span><span class="caption-text">Results on Caltech 101 Dataset.</span><a class="headerlink" href="#lazebnikresults2" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="intermediate-summary-on-spatial-pyramid-matching">
<h4>Intermediate Summary on Spatial Pyramid Matching<a class="headerlink" href="#intermediate-summary-on-spatial-pyramid-matching" title="Permalink to this headline">¶</a></h4>
<p>As depicted below, Spatial Pyramid Matching integrates an additional layer to the previous BoVW, which provides spatial information.</p>
<figure class="align-center" id="spmlayers">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/layerSchemaSPM.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/layerSchemaSPM.png" src="https://maucher.home.hdm-stuttgart.de/Pics/layerSchemaSPM.png" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 23 </span><span class="caption-text">Overview: Spatial Pyramid Matching with SVM classification</span><a class="headerlink" href="#spmlayers" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>As the results above prove, SPM+SVM yields a much better performance than BoW. Correspondingly, this technique has been widely applied and it also constitutes the base for a bunch of future improvements. The major drawbacks of the approach as implemented in <span id="pending-xref-22">[<a class="reference internal" href="../referenceSection.html#citation-31">LSP06</a>]</span></p>
<ol class="simple">
<li><p>The <strong>SVM Kernel</strong> applied in equation <a class="reference internal" href="#equation-svmspm">(21)</a>, is <strong>non-linear</strong>. For non-linear SVMs the complexity is</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathcal{O}(N^3)\)</span> (for training computation)</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{O}(N^2)\)</span> (memory)</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathcal{O}(N)\)</span> (for testing computation)
where N is the number of training images</p></li>
</ul>
</li>
<li><p><strong>Hard Encoding:</strong> The implemented encoding (vector quantization) assign each local descriptor to exactly on visual word (the nearest cluster-center). However, it may be better if descriptors, which are located at the cluster-borders are assigned to all the nearby clusters.</p></li>
</ol>
<p>In <span id="pending-xref-23">[<a class="reference internal" href="../referenceSection.html#citation-51">YYGTHuang09</a>]</span> an extension has been developed, that allows <strong>linear SVMs</strong>. This approach not only decreases complexity but increases classification accuracy. A key element of this approach is the use of <strong>sparse coding</strong> instead of vector quantisation. In the following subsections the integration of sparse coding and linear SVM classification into the SPM stack is described.</p>
</section>
</section>
<section id="sparse-coding">
<h3>Sparse Coding<a class="headerlink" href="#sparse-coding" title="Permalink to this headline">¶</a></h3>
<p>Vector quantisation, as applied in the context of K-Means clustering, maps each vector in the d-dimensional space to the closest cluster-center. This implies that each vector is assigned to exactly one cluster. As depicted in the figure below, this type of encoding ignores the fact that some vectors may be close to one center and far away from all other centers, whereas other vectors may have nearly the same distance to 2 or more centers. In the latter case the hard assignment to one cluster may be disruptive. Sparse coding solves this problem by providing the possibility, that vectors at the cluster boundaries, can be assigned to more than one cluster. Moreover, for all the assigned clusters also weights are calculated, such that closer cluster-centers have a higher weight than cluster-centers, which are further away.</p>
<figure class="align-center" id="sparsecodingidea">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/vq2sparseCoding.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/vq2sparseCoding.png" src="https://maucher.home.hdm-stuttgart.de/Pics/vq2sparseCoding.png" style="width: 650px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 24 </span><span class="caption-text">Vector Quantisation (left) and Sparse Coding (right)</span><a class="headerlink" href="#sparsecodingidea" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>In the remainder of this subsection the training- and inference-phase of a standard sparse coding approach is described. In order to better understand this approach it makes sense to first describe k-means clustering in the <em>language</em>, which will be applied for describing sparse coding. In this way it should become obvious, how sparse coding modifies and extends k-means.</p>
<section id="new-notation-for-describing-k-means">
<h4>New notation for describing k-means<a class="headerlink" href="#new-notation-for-describing-k-means" title="Permalink to this headline">¶</a></h4>
<p>In order to describe sparse coding, we apply the following notation:</p>
<ul>
<li><p>The <span class="math notranslate nohighlight">\(N\)</span> training samples are the rows of the matrix<a class="footnote-reference brackets" href="#footnote2" id="footnote-reference-2" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a></p>
<div class="math notranslate nohighlight" id="equation-notx">
<span class="eqno">(22)<a class="headerlink" href="#equation-notx" title="Permalink to this equation">¶</a></span>\[
	X=\left[\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_N \right]^T
	\]</div>
</li>
<li><p>The <span class="math notranslate nohighlight">\(K\)</span> cluster centers are the rows of the matrix</p>
<div class="math notranslate nohighlight" id="equation-notv">
<span class="eqno">(23)<a class="headerlink" href="#equation-notv" title="Permalink to this equation">¶</a></span>\[
	V=\left[\mathbf{v}_1,\mathbf{v}_2,\ldots,\mathbf{v}_K \right]^T
	\]</div>
<p>Matrix <span class="math notranslate nohighlight">\(V\)</span> is also called codebook.</p>
</li>
<li><p>The rows of the <span class="math notranslate nohighlight">\(N \times K\)</span> matrix</p>
<div class="math notranslate nohighlight" id="equation-notu">
<span class="eqno">(24)<a class="headerlink" href="#equation-notu" title="Permalink to this equation">¶</a></span>\[
	U=\left[\mathbf{u}_1,\mathbf{u}_2,\ldots,\mathbf{u}_N \right]^T
	\]</div>
<p>define the cluster membership of the training sampels, i.e. if the <span class="math notranslate nohighlight">\(i.th\)</span> training vector <span class="math notranslate nohighlight">\(\mathbf{x}_i\)</span> belongs to cluster <span class="math notranslate nohighlight">\(j\)</span>, then the <span class="math notranslate nohighlight">\(j.th\)</span> component of <span class="math notranslate nohighlight">\(\mathbf{u}_i\)</span> is <span class="math notranslate nohighlight">\(1\)</span> and all other components are <span class="math notranslate nohighlight">\(0\)</span>.</p>
</li>
<li><p><strong>L2-Norm</strong> of vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
	\Vert \mathbf{x} \Vert =  \sqrt{\sum\limits_{i=1}^d x_i^2}
	\]</div>
</li>
<li><p><strong>L1-Norm</strong> of vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
	| \mathbf{x} | = \sum\limits_{i=1}^d |x_i|
	\]</div>
</li>
</ul>
<p>With this notation in mind, the <strong>K-means algorithm</strong> can also be described as follows:</p>
<p>K-means clustering <strong>finds the codebook <span class="math notranslate nohighlight">\(V\)</span> such that the reconstruction error (equation <a class="reference internal" href="#equation-recerror">(15)</a>) is minimized</strong>, i.e. k-means solves the following optimisation problem</p>
<div class="math notranslate nohighlight">
\[
\min\limits_V \sum\limits_{m=1}^N \min\limits_{k=1,\ldots,K} \Vert \mathbf{x}_m - \mathbf{v}_k \Vert ^2
\]</div>
<p>This optimisation problem can be re-formulated as the following matrix factorisation problem</p>
<div class="math notranslate nohighlight" id="equation-kopt">
<span class="eqno">(25)<a class="headerlink" href="#equation-kopt" title="Permalink to this equation">¶</a></span>\[
\min\limits_{U,V} \sum\limits_{m=1}^N  \Vert \mathbf{x}_m - \mathbf{u}_m V \Vert ^2 \quad \mbox{ subject to } \quad Card(\mathbf{u}_m)=1, |\mathbf{u}_m|=1, \mathbf{u}_m \succeq 0 %\; \forall m ,
\]</div>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( Card(\mathbf{u}_m)=1\)</span> means that only one element of <span class="math notranslate nohighlight">\(\mathbf{u}_m\)</span> is nonzero</p></li>
<li><p><span class="math notranslate nohighlight">\(|\mathbf{u}_m|=1\)</span> means that the <span class="math notranslate nohighlight">\(L1\)</span>-norm is 1.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{u}_m \succeq 0\)</span> means that all elements in <span class="math notranslate nohighlight">\(\mathbf{u}_m\)</span> are nonnegative</p></li>
</ul>
<p>In the k-means <strong>training phase</strong> the optimisation problem of equation <a class="reference internal" href="#equation-kopt">(25)</a> is solved w.r.t. <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(V\)</span>. In the <strong>encoding phase</strong>, the learned codebook <span class="math notranslate nohighlight">\(V\)</span> is fixed and equation <a class="reference internal" href="#equation-kopt">(25)</a> will be solved w.r.t. <span class="math notranslate nohighlight">\(U\)</span> for a new dataset <span class="math notranslate nohighlight">\(X\)</span>.</p>
</section>
<section id="from-k-means-to-sparse-coding">
<h4>From k-means to sparse-coding<a class="headerlink" href="#from-k-means-to-sparse-coding" title="Permalink to this headline">¶</a></h4>
<p>The step from k-means to sparse coding can be done by relaxing the condition <span class="math notranslate nohighlight">\(Card(\mathbf{u_m})=1\)</span> in equation <a class="reference internal" href="#equation-kopt">(25)</a>. I.e. instead of requiring, that each input is mapped to exactly one cluster, <strong>we allow inputs to be assigned to more than one, but only a few clusters</strong>. The condition <em>one or only a few</em> can be realized by <strong>L1-regularisation</strong>, which is defined in this context as follows:</p>
<div class="math notranslate nohighlight" id="equation-sparsecoding">
<span class="eqno">(26)<a class="headerlink" href="#equation-sparsecoding" title="Permalink to this equation">¶</a></span>\[
\min\limits_{U,V} \sum\limits_{m=1}^N \Vert \mathbf{x}_m - \mathbf{u}_m V \Vert ^2 + \lambda|\mathbf{u}_m| \quad \mbox{ subject to } \Vert \mathbf{v}_k \Vert = 1 \; \forall k
\]</div>
<p>The <span class="math notranslate nohighlight">\(L1\)</span>-norm regularization enforces <span class="math notranslate nohighlight">\(\mathbf{u}_m\)</span> to have a small number of nonzero elements. Moreover, the constraint <span class="math notranslate nohighlight">\(\Vert \mathbf{v}_k \Vert = 1\)</span> provides normalisation of the codebook.</p>
<p><strong>Coordinate Descent Algorithm:</strong></p>
<p>The sparse optimisation problem of equation <a class="reference internal" href="#equation-sparsecoding">(26)</a> can be solved e.g. by the <a class="reference external" href="http://en.wikipedia.org/wiki/Coordinate%5C_descent">Coordinate Descent Algorithm</a>. This algorithms minimizes a multivariate function <span class="math notranslate nohighlight">\(f(\mathbf{x})=f(x_1,x_2,\ldots,x_n)\)</span> as follows:</p>
<ol>
<li><p>Start at an arbitrary point <span class="math notranslate nohighlight">\(\mathbf{x}^0=(x_1^0,x_2^0,\ldots,x_n^0)\)</span></p></li>
<li><p>Set <span class="math notranslate nohighlight">\(k=1,i=1\)</span>: The index <span class="math notranslate nohighlight">\(i\)</span> defines along which coordinate <span class="math notranslate nohighlight">\(x_i\)</span> the function <span class="math notranslate nohighlight">\(f(\mathbf{x})=f(x_1,x_2,\ldots,x_n)\)</span> is minimized in round <span class="math notranslate nohighlight">\(k\)</span></p></li>
<li><p>In round <span class="math notranslate nohighlight">\(k\)</span> determine a new value for <span class="math notranslate nohighlight">\(x_i^{k}\)</span>. This new value for <span class="math notranslate nohighlight">\(x_i^{k}\)</span> minimizes the function along the <span class="math notranslate nohighlight">\(x_i\)</span>-axis:</p>
<div class="math notranslate nohighlight">
\[
	x_i^{k}=\arg \min \limits_{y \in \cal{R}} f(x_1^{k-1},x_2^{k-1},\ldots,x_{i-1}^{k-1},y,x_{i+1}^k,\ldots,x_n^k)
	\]</div>
<p>For all other axis <span class="math notranslate nohighlight">\(x_j, j \neq  i\)</span> the values <span class="math notranslate nohighlight">\(x_j\)</span> remain unchanged, i.e. <span class="math notranslate nohighlight">\(x_j^k=x_j^{k-1}\)</span> in this iteration</p>
</li>
<li><p>Set <span class="math notranslate nohighlight">\(i:=(i+1)\mod n\)</span> and <span class="math notranslate nohighlight">\(k:=k+1\)</span></p></li>
<li><p>Go to 3. as long as termination criteria is not fulfilled</p></li>
</ol>
<p>It is guaranteed that from iteration to iterartion the function contiuously decreases until a local minima is reached.</p>
<div class="math notranslate nohighlight">
\[
f(\mathbf{x}^0) \geq f(\mathbf{x}^1) \geq f(\mathbf{x}^2) \geq \ldots
\]</div>
<figure class="align-center" id="coordinatedescent">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/CoordinateDescent.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/CoordinateDescent.png" src="https://maucher.home.hdm-stuttgart.de/Pics/CoordinateDescent.png" style="width: 550px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 25 </span><span class="caption-text">In each iteration the coordinate-descent-algorithms minimizes the function along only one axis.</span><a class="headerlink" href="#coordinatedescent" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>Other popular sparse coding techniques are e.g. <strong>sparse Restricted Boltzman Machines (RBM)</strong> and <strong>sparse Autoencoders</strong>. Both can be
realized as neural networks.</p>
</section>
</section>
<section id="replace-non-linear-by-linear-svm">
<h3>Replace non-linear by linear SVM<a class="headerlink" href="#replace-non-linear-by-linear-svm" title="Permalink to this headline">¶</a></h3>
<p>As already mentioned above, in <span id="pending-xref-24">[<a class="reference internal" href="../referenceSection.html#citation-51">YYGTHuang09</a>]</span> an extension to the Spatial Pyramid Matching approach of <span id="pending-xref-25">[<a class="reference internal" href="../referenceSection.html#citation-31">LSP06</a>]</span> has been developed, which integrates <strong>linear SVMs</strong> and <strong>sparse coding</strong> in the overall stack. This new stack is depicted below:</p>
<figure class="align-center" id="layerspm">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/layerSchemaLinearSPM.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/layerSchemaLinearSPM.png" src="https://maucher.home.hdm-stuttgart.de/Pics/layerSchemaLinearSPM.png" style="width: 650px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 26 </span><span class="caption-text">The new approach integrates Sparse Coding, Pooling and a linear SVM</span><a class="headerlink" href="#layerspm" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>The application of Sparse Coding, Pooling and linear SVM in the context of this stack is described below:</p>
<section id="sparse-coding-for-generating-mid-level-features">
<h4>Sparse Coding for generating mid level features<a class="headerlink" href="#sparse-coding-for-generating-mid-level-features" title="Permalink to this headline">¶</a></h4>
<p><strong>Training of Sparse Coder:</strong></p>
<ul class="simple">
<li><p>Let <span class="math notranslate nohighlight">\(X\)</span> be the set of <span class="math notranslate nohighlight">\(N\)</span> training descriptors, as defined in equation <a class="reference internal" href="#equation-notx">(22)</a></p></li>
<li><p>Solve equation <a class="reference internal" href="#equation-sparsecoding">(26)</a> w.r.t. <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(V\)</span> for the given training images, where <span class="math notranslate nohighlight">\(U\)</span> and <span class="math notranslate nohighlight">\(V\)</span> are defined as in equation <a class="reference internal" href="#equation-notu">(24)</a> and <a class="reference internal" href="#equation-notv">(23)</a>, respectively.</p></li>
</ul>
<p><strong>Inference Phase Sparse Coder:</strong></p>
<ul class="simple">
<li><p>Let <span class="math notranslate nohighlight">\(X\)</span> be the matrix of <span class="math notranslate nohighlight">\(M\)</span> descriptors of the new image.</p></li>
<li><p><span class="math notranslate nohighlight">\(V\)</span> is the codebook determined in the training phase.</p></li>
<li><p>Solve equation <a class="reference internal" href="#equation-sparsecoding">(26)</a> w.r.t. <span class="math notranslate nohighlight">\(U\)</span>.</p></li>
<li><p>The <span class="math notranslate nohighlight">\(i.th\)</span> row of <span class="math notranslate nohighlight">\(U\)</span> determines which codewords (mid level features) of the codebook <span class="math notranslate nohighlight">\(V\)</span> are activated by the <span class="math notranslate nohighlight">\(i.th\)</span> descriptor in <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p>The <span class="math notranslate nohighlight">\(j.th\)</span> column determines the responses of all descriptors in <span class="math notranslate nohighlight">\(X\)</span> to one specific mid level feature (codeword) in <span class="math notranslate nohighlight">\(V\)</span>.</p></li>
</ul>
</section>
<section id="maxpooling">
<h4>Maxpooling<a class="headerlink" href="#maxpooling" title="Permalink to this headline">¶</a></h4>
<ul class="simple">
<li><p>Let <span class="math notranslate nohighlight">\(U\)</span> be the sparse code of descriptor set <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p>For each of the <span class="math notranslate nohighlight">\(K\)</span> columns in matrix <span class="math notranslate nohighlight">\(U\)</span> calculate the <span class="math notranslate nohighlight">\(j.th\)</span> component of vector <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> as</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
z_j=\max \lbrace |u_{1,j}|, |u_{2,j}|, \ldots, |u_{M,j}|\rbrace
\]</div>
<ul class="simple">
<li><p><strong>The value of <span class="math notranslate nohighlight">\(z_j\)</span> describes the presence of mid-level feature <span class="math notranslate nohighlight">\(\mathbf{v}_j\)</span></strong> in the corresponding spatial region of the image, described by <span class="math notranslate nohighlight">\(X\)</span>.</p></li>
<li><p>Note, that in the original Spatial Pyramid Matching approach by <span id="pending-xref-26">[<a class="reference internal" href="../referenceSection.html#citation-31">LSP06</a>]</span> the representation <span class="math notranslate nohighlight">\(z\)</span> is a histogram, which counts the frequency of a mid-level feature <span class="math notranslate nohighlight">\(\mathbf{v}_j\)</span> in the corresponding spatial region of the image.</p></li>
</ul>
<p><strong>Max Pooling in the context of Spatial Pyramid Matching:</strong></p>
<p>Similar to the construction of histograms in SPM, Max Pooling Spatial Pyramids are constructed by applying max pooling across different locations and over different spatial scales. Max-Pooled features from different locations and scales are then concatenated as in SPM.</p>
<figure class="align-center" id="linearspm">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/yangSCandPooling.PNG"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/yangSCandPooling.PNG" src="https://maucher.home.hdm-stuttgart.de/Pics/yangSCandPooling.PNG" style="width: 450px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 27 </span><span class="caption-text">Integration of Max Pooling in Spatial Pyramid Matching</span><a class="headerlink" href="#linearspm" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="integrate-linear-svm">
<h4>Integrate Linear SVM<a class="headerlink" href="#integrate-linear-svm" title="Permalink to this headline">¶</a></h4>
<ul>
<li><p>Let <span class="math notranslate nohighlight">\(\mathbf{z}_i\)</span> be the max pooling representation of image <span class="math notranslate nohighlight">\(I_i\)</span>.</p></li>
<li><p>The <strong>linear kernel</strong> in the SVM is</p>
<div class="math notranslate nohighlight">
\[
	\kappa(\mathbf{z}_i,\mathbf{z}_j) = \mathbf{z}_i^T \mathbf{z}_j = \sum\limits_{\ell=0}^2 \sum\limits_{s=0}^{2^\ell} \sum\limits_{t=0}^{2^\ell} \mathbf{z}_i^T(\ell,s,t) \mathbf{z}_j(\ell,s,t),
	\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{z}_i^T(\ell,s,t)\)</span> is the max-pooling output of the spatial segment at position <span class="math notranslate nohighlight">\((s,t)\)</span> at level <span class="math notranslate nohighlight">\(\ell\)</span>.</p>
</li>
<li><p>The binary decision function of the SVM is then</p>
<div class="math notranslate nohighlight">
\[
	f(\mathbf{z}) = \left(\sum\limits_{i=1}^N \alpha_i y_i \mathbf{z}_i \right)^T \mathbf{z} + b = \mathbf{w}^T \mathbf{z} + b
	\]</div>
</li>
</ul>
</section>
<section id="performance-of-the-linear-spatial-pyramid-matching-approach">
<h4>Performance of the linear Spatial Pyramid Matching approach<a class="headerlink" href="#performance-of-the-linear-spatial-pyramid-matching-approach" title="Permalink to this headline">¶</a></h4>
<p>The computational <strong>training cost is <span class="math notranslate nohighlight">\(\mathcal{O}(N)\)</span></strong>, testing cost is constant (independent of <span class="math notranslate nohighlight">\(N\)</span>). According to the authors of <span id="pending-xref-27">[<a class="reference internal" href="../referenceSection.html#citation-51">YYGTHuang09</a>]</span> <strong>Linear SPM kernel based on sparse coding statistics always achieves excellent classification accuracy</strong>, because</p>
<ul class="simple">
<li><p>Sparse Coding has much less quantization errors than vector quantisation (k-means)</p></li>
<li><p>The computed statistics by max pooling are more salient and robust to local translations</p></li>
</ul>
<p>On the Caltech-101 dataset the classification accuracy of this approach is <strong>about <span class="math notranslate nohighlight">\(10\%\)</span> better</strong>, than the SPM approach of <span id="pending-xref-28">[<a class="reference internal" href="../referenceSection.html#citation-31">LSP06</a>]</span>.</p>
<figure class="align-center" id="linear2spm">
<a class="reference internal image-reference" href="https://maucher.home.hdm-stuttgart.de/Pics/linearSPM.png"><img alt="https://maucher.home.hdm-stuttgart.de/Pics/linearSPM.png" src="https://maucher.home.hdm-stuttgart.de/Pics/linearSPM.png" style="width: 550px;" /></a>
</figure>
<ul class="simple">
<li><p>KSPM: Spatial Pyramid Matching with K-means, histogram pooling and non-linear SVM <span id="pending-xref-29">[<a class="reference internal" href="../referenceSection.html#citation-31">LSP06</a>]</span></p></li>
<li><p>LSPM: Spatial Pyramid Matching with K-means, histogram pooling and linear SVM</p></li>
<li><p>ScSPM: Spatial Pyramid Matching with Sparse Coding, maxpooling and linear SVM <span id="pending-xref-30">[<a class="reference internal" href="../referenceSection.html#citation-51">YYGTHuang09</a>]</span>.</p></li>
</ul>
<hr class="footnotes docutils" />
<aside class="footnote brackets" id="footnote1" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#footnote-reference-1">1</a><span class="fn-bracket">]</span></span>
<p><strong>Mean Rank</strong> is the mean position of the correct label, when labels are sorted in decreasing order w.r.t. the classifier score, as calculated in equation <a class="reference internal" href="#equation-eq-nbred">(17)</a></p>
</aside>
<aside class="footnote brackets" id="footnote2" role="note">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#footnote-reference-2">2</a><span class="fn-bracket">]</span></span>
<p>Keep in mind that the columns of matrix <span class="math notranslate nohighlight">\(X\)</span> are the rows of the transpose <span class="math notranslate nohighlight">\(X^T\)</span></p>
</aside>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./recognition"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="../features/HOGpedestrianDetection.html" title="previous page">HOG-based Pedestrian Detection</a>
    <a class='right-next' id="next-link" href="../deeplearning/ConvolutionNeuralNetworks.html" title="next page">Convolutional Neural Networks</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Prof. Dr. Johannes Maucher<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>